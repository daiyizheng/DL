{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libsvm 使用SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import libsvm\n",
    "from libsvm import svm\n",
    "from libsvm.svmutil import svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集下载：https://pan.baidu.com/s/1ZkxGIvvGml3vig-9_s1pRw\n",
    "## 百度网盘下载加速：https://www.baiduwp.com/?m=index\n",
    "news_file='../data/SVM/cnews.train.txt'         ##原始是数据\n",
    "test_file='../data/SVM/cnews.test.txt'          ##测试数据\n",
    "output_word_file='cnews_dict.txt'   ##进过分词后的数\n",
    "output_word_test_file='cnews_dict_test.txt'\n",
    "feature_file='cnews_feature_file.txt'             ##最后生成的词向量文件\n",
    "feature_test_file='cnews_feature_test_file.txt'\n",
    "model_filename='cnews_model'                     ##模型保存的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(news_file, 'r') as f:       ##读取新闻文章\n",
    "    lines = f.readlines()\n",
    "\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "print(content)\n",
    "\n",
    "words_iter = jieba.cut(content)          ##使用jiejia进行分词操作\n",
    "print('/ '.join(words_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_file(input_char_file, output_word_file):        ##定义分词函数，并写入文件\n",
    "    with open(input_char_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_word_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            words_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in words_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != '':\n",
    "                    word_content += word + ' '\n",
    "            out_line = '%s\\t%s\\n' % (label, word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "\n",
    "generate_word_file(news_file, output_word_file)\n",
    "generate_word_file(test_file, output_word_test_file)\n",
    "print('==========分词完成====================')            ##需要的时间比较长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:               ##分类topic\n",
    "    def __init__(self, category_file):\n",
    "        self._category_to_id = {}\n",
    "        with open(category_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category, idx = line.strip('\\r\\n').split('\\t')\n",
    "            idx = int(idx)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        return self._category_to_id[category]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "\n",
    "category_file='cnews.category.txt'\n",
    "category_vocab = Category(category_file)\n",
    "print(category_vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##对分词后的数据进行词频统计并过滤，分配词ID\n",
    "\n",
    "def generate_feature_dict(train_file, feature_threshold=10):   \n",
    "    feature_dict = {}\n",
    "    with open(train_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split(' '):\n",
    "            if not word in feature_dict:\n",
    "                feature_dict.setdefault(word, 0)\n",
    "            feature_dict[word] += 1\n",
    "    filtered_feature_dict = {}\n",
    "    for feature_name in feature_dict:\n",
    "        if feature_dict[feature_name] < feature_threshold:\n",
    "            continue\n",
    "        if not feature_name in filtered_feature_dict:\n",
    "            filtered_feature_dict[feature_name] = len(filtered_feature_dict) + 1\n",
    "    return filtered_feature_dict\n",
    "        \n",
    "feature_dict = generate_feature_dict(output_word_file, feature_threshold=200)\n",
    "print(len(feature_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_line(line, feature_dict, category_vocab):     ##对每一篇文章根据词id构造词向量。\n",
    "    label, content = line.strip('\\r\\n').split('\\t')\n",
    "    label_id = category_vocab.category_to_id(label)\n",
    "    feature_example = {}\n",
    "    for word in content.split(' '):\n",
    "        if not word in feature_dict:\n",
    "            continue\n",
    "        feature_id = feature_dict[word]\n",
    "        feature_example.setdefault(feature_id, 0)\n",
    "        feature_example[feature_id] += 1\n",
    "    feature_line = '%d' % label_id\n",
    "    sorted_feature_example = sorted(feature_example.items(), key=lambda d:d[0])\n",
    "    for item in sorted_feature_example:\n",
    "        feature_line += ' %d:%d' % item\n",
    "    return feature_line\n",
    "\n",
    "##循环没一篇文章，得到词向量化后的文件\n",
    "\n",
    "def convert_raw_to_feature(raw_file, feature_file, feature_dict, category_vocab):   \n",
    "    with open(raw_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(feature_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            feature_line = generate_feature_line(line, feature_dict, category_vocab)\n",
    "            f.write('%s\\n' % feature_line)\n",
    "            \n",
    "##测试数据运用相同的词ID表\n",
    "convert_raw_to_feature(output_word_file, feature_file, feature_dict, category_vocab)\n",
    "convert_raw_to_feature(output_word_test_file, feature_test_file, feature_dict, category_vocab)  \n",
    "print('==========构造词向量完成完成====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##生成svm训练数据\n",
    "train_label, train_value = svm_read_problem(feature_file)\n",
    "print(train_label[0],train_value[0])\n",
    "train_test_label, train_test_value = svm_read_problem(feature_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(model_filename)):                ##判断模型是否存在，存在直接读取\n",
    "    model=svm_load_model(model_filename)\n",
    "else:\n",
    "    model=svm_train(train_label,train_value,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')   ##模型训练\n",
    "    svm_save_model(model_filename,model)                    \n",
    "print(\"=======模型训练完成================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##模型预测，并打印出精确度。\n",
    "p_labs, p_acc, p_vals =svm_predict(train_test_label, train_test_value, model)   \n",
    "print(p_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit learn 使用SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear')\n",
      "[[1. 1.]\n",
      " [2. 3.]]\n",
      "[1 2]\n",
      "[1 1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#训练样本\n",
    "x = [[2,0], [1,1], [2,3]]\n",
    "#label\n",
    "y = [0,0,1]\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "clf.fit(x, y)\n",
    "\n",
    "#打印出参数设置情况,只设置了 kernel，其他都是默认\n",
    "print (clf)\n",
    "\n",
    "#支持向量\n",
    "print (clf.support_vectors_)\n",
    "\n",
    "#支持向量的index\n",
    "print (clf.support_)\n",
    "\n",
    "#对于每个类别，分别有几个支持向量\n",
    "print (clf.n_support_)\n",
    "\n",
    "#对新数据进行预测\n",
    "print (clf.predict([[2,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
