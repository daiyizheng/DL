{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DYZ/dyz1/anaconda3/envs/cv_project/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f666c6bc810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f65716b4110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = datasets.MNIST(\"./datasets/\", \n",
    "                             download=True, \n",
    "                             train=True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(), \n",
    "                            ]))\n",
    "plt.imshow(np.transpose(sample_data[0][0].numpy(), (1,2,0)), \n",
    "           cmap='Greys_r') # [channel, height, width] -> [height, width, channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "pin_memory = True\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "device = torch.device(\"cuda\")\n",
    "epochs = 1\n",
    "num_diffusion_timestep = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32), \n",
    "                      interpolation=transforms.InterpolationMode.BICUBIC, \n",
    "                      antialias=True),#插值的方法\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"./datasets/\", \n",
    "                         download=True, \n",
    "                         train=True,\n",
    "                         transform=transform)\n",
    "\n",
    "test_data = datasets.MNIST('./datasets/', \n",
    "                           train=False, \n",
    "                           download=True, \n",
    "                           transform=transform)\n",
    "# dataset[0]\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          pin_memory=pin_memory, \n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         pin_memory=pin_memory,\n",
    "                         num_workers=num_workers,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， 我们对图像进行加噪声\n",
    "初始化超参数\n",
    "$\\beta_t$                       \n",
    "$\\alpha_t = 1-\\beta_t$      \n",
    "$\\sqrt{\\alpha_t} = \\sqrt{1-\\beta_t}$                                                \n",
    "$\\overline{\\alpha}=\\prod_{t=1}^{T}\\alpha_t$     \n",
    "$\\sqrt{1-\\overline{\\alpha}}$        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffusion_setting:\n",
    "    def __init__(self, num_diffusion_timestep) -> None:\n",
    "        self.timestep = num_diffusion_timestep\n",
    "        self.initialize()\n",
    "    \n",
    "    def initialize(self):\n",
    "        # calculate all the settings for every timesteps, store in a tensor matrix\n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        self.beta = self.get_beta()\n",
    "        self.alpha = 1-self.beta\n",
    "        self.one_by_sqrt_alpha_s = 1. / torch.sqrt(self.alpha)\n",
    "        self.sqrt_beta_s = torch.sqrt(self.beta)  # the var for denoising\n",
    "        \n",
    "        # a single calculated cumulative values\n",
    "        self.alpha_cumulative = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sqrt_alpha_cumulative = torch.sqrt(self.alpha_cumulative)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
    "        \n",
    "    \n",
    "    def get_beta(self):\n",
    "        ## linear schedule, following original ddpm paper\n",
    "        scale = 1000/ self.timestep\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, \n",
    "                              beta_end, \n",
    "                              self.timestep,\n",
    "                              dtype=torch.float32, \n",
    "                              device=device)\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_t=\\sqrt{\\overline{\\alpha_t}} x_0+\\sqrt{1-\\overline{\\alpha_t}} z_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(DS, \n",
    "                      X_0, \n",
    "                      batch_timestep):\n",
    "    \"\"\" \n",
    "    前向加噪声过程\n",
    "    \"\"\"\n",
    "    # sample a batch of Noise ~ N(0,1)\n",
    "    z_t = torch.randn_like(X_0) # [bz, c, w, h]\n",
    "    \n",
    "    # Images scaled to X_t_batch/sqrt(alpha_t)\n",
    "    batch_sqrt_alpha_cumulative = torch.gather(input=DS.sqrt_alpha_cumulative, \n",
    "                                               index=batch_timestep, \n",
    "                                               dim=0) # 维度 [bz]\n",
    "    mean = batch_sqrt_alpha_cumulative.reshape(-1, 1, 1, 1) * X_0\n",
    "    batch_sqrt_one_minus_alpha_cumulative = torch.gather(input=DS.sqrt_one_minus_alpha_cumulative, \n",
    "                                                         index=batch_timestep,\n",
    "                                                         dim=0).reshape(-1, 1, 1, 1)\n",
    "    return mean + batch_sqrt_one_minus_alpha_cumulative*z_t, z_t\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义正弦余弦位置信息\n",
    "$$\\begin{aligned}\n",
    "& P E_{(\\mathrm{pos}, 2 i)}=\\sin \\left(\\operatorname{pos} / 10000^{2 i / d_{\\text {model }}}\\right) \\\\\n",
    "& P E_{(p o s, 2 i+1)}=\\cos \\left(\\operatorname{pos} / 10000^{2 i / d_{\\text {model }}}\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        half_dim =  time_emb_dims//2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        self.time_block = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp)\n",
    "        )\n",
    "    def forward(self, time):\n",
    "        return self.time_block(time)\n",
    " \n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__() \n",
    "        self.downsample = nn.Conv2d(in_channels=channels, \n",
    "                                    out_channels=channels, \n",
    "                                    kernel_size=3, \n",
    "                                    stride=2, \n",
    "                                    padding=1)  \n",
    "    def forward(self, x, *args):\n",
    "        return self.downsample(x)\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=in_channels, \n",
    "                      kernel_size=3, \n",
    "                      stride=1, \n",
    "                      padding=1))\n",
    "        \n",
    "    def forward(self, x, *args):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels=64) -> None:\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        h = h.reshape(B, self.channels, H*W).swapaxes(1, 2) # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
    "        h, _ = self.mhsa(h, h, h)\n",
    "        h = h.swapaxes(2, 1).view(B, self.channels, H, W) # [B, C, H*W] --> [B, C, H, W]\n",
    "        return x+h\n",
    "\n",
    "  \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 dropout_rate=0.1, \n",
    "                 time_emb_dims=512, \n",
    "                 apply_attention=False) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.act_fn = nn.SiLU()\n",
    "        \n",
    "        # Group 1\n",
    "        self.normlize_1 = nn.GroupNorm(num_groups=8, \n",
    "                                       num_channels=self.in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                out_channels=self.out_channels, \n",
    "                                kernel_size=3, \n",
    "                                stride=1, \n",
    "                                padding=\"same\")\n",
    "        \n",
    "        # Group 2 time embedding\n",
    "        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n",
    "        \n",
    "        # Group 3\n",
    "        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, \n",
    "                                out_channels=self.out_channels, \n",
    "                                kernel_size=3, \n",
    "                                stride=1, \n",
    "                                padding=\"same\")\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.match_input = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                         out_channels=self.out_channels, \n",
    "                                         kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.match_input = nn.Identity()\n",
    "        \n",
    "        if apply_attention:\n",
    "            self.attention = AttentionBlock(channels=self.out_channels)\n",
    "        else:\n",
    "            self.attention = nn.Identity()  \n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # group 1\n",
    "        h = self.act_fn(self.normlize_1(x))\n",
    "        h = self.conv_1(h)\n",
    "        \n",
    "        # group 2 \n",
    "        # add in timestep embedding\n",
    "        h += self.dense_1(self.act_fn(t))[:, :, None, None] #\n",
    "        \n",
    "        # group 3\n",
    "        h = self.act_fn(self.normlize_2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv_2(h)\n",
    "        \n",
    "        # Residual and attention\n",
    "        h = h + self.match_input(x)\n",
    "        h = self.attention(h) \n",
    "         \n",
    "        return h\n",
    "        \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_channels=3,\n",
    "                 output_channels=3,\n",
    "                 num_res_blocks=2,\n",
    "                 base_channels=128,\n",
    "                 base_channels_multiples=(1, 2, 4, 8),\n",
    "                 apply_attention=(False, False, True, False),\n",
    "                 dropout_rate=0.1,\n",
    "                 time_multiple=4,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        time_emb_dims_exp = base_channels * time_multiple # 32 * 4=128\n",
    "        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels,\n",
    "                                                            time_emb_dims_exp=time_emb_dims_exp)\n",
    "        \n",
    "        self.first = nn.Conv2d(in_channels=input_channels, \n",
    "                               out_channels=base_channels,\n",
    "                               kernel_size=3, stride=1, padding=\"same\")\n",
    "        \n",
    "        num_resolutions = len(base_channels_multiples) \n",
    "        \n",
    "        # Encoder part of The Unet. Dimension reduction\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        curr_channels = [base_channels]\n",
    "        in_channels = base_channels\n",
    "        \n",
    "        for level in range(num_resolutions):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "            \n",
    "            for _ in range(num_res_blocks):\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level]\n",
    "                )\n",
    "                self.encoder_blocks.append(block)\n",
    "                \n",
    "                in_channels = out_channels\n",
    "                curr_channels.append(in_channels)\n",
    "            \n",
    "            if level != (num_resolutions-1):\n",
    "                self.encoder_blocks.append(DownSample(channels=in_channels))\n",
    "                curr_channels.append(in_channels)\n",
    "        \n",
    "        # Bottleneck in between\n",
    "        self.bottleneck_block = nn.ModuleList(\n",
    "            (\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels, \n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=True\n",
    "                ),\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=False,\n",
    "                )\n",
    "            )\n",
    "        ) \n",
    "        \n",
    "        # Decoder part of the Unet. Dimension restoration with skip-connections. \n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        for level in reversed(range(num_resolutions)):\n",
    "            \n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "            \n",
    "            for _ in range(num_res_blocks+1):\n",
    "                encoder_in_channels = curr_channels.pop()\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=encoder_in_channels+in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level]\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "                self.decoder_blocks.append(block)\n",
    "            \n",
    "            if level !=0:\n",
    "                self.decoder_blocks.append(UpSample(in_channels=in_channels))\n",
    "        \n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "        )\n",
    "                     \n",
    "    def forward(self, x, t):\n",
    "        time_emb = self.time_embeddings(t)\n",
    "        \n",
    "        h = self.first(x)\n",
    "        outs = [h]\n",
    "        \n",
    "        for layer in self.encoder_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "            outs.append(h)\n",
    "        \n",
    "        for layer in self.bottleneck_block:\n",
    "            h = layer(h, time_emb)\n",
    "            \n",
    "        for layer in self.decoder_blocks:\n",
    "            if isinstance(layer, ResnetBlock):\n",
    "                out = outs.pop()\n",
    "                h = torch.cat([h, out], dim=1)\n",
    "            h = layer(h, time_emb)\n",
    "            \n",
    "        h = self.final(h)\n",
    "        \n",
    "        \n",
    "        return h       \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(input_channels=1, \n",
    "             output_channels=1, \n",
    "             base_channels=64, \n",
    "             base_channels_multiples=(1, 2, 4, 8),\n",
    "             apply_attention=(False, False, True, False),\n",
    "             dropout_rate=0.1,\n",
    "             time_multiple=2)\n",
    "model = model.to(device)\n",
    "# batch_timesteps = torch.arange(128)\n",
    "# X_train = torch.randn((128, 1, 32, 32))\n",
    "# model(X_train, batch_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = diffusion_setting(num_diffusion_timestep=num_diffusion_timestep)   \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定扩散过程任意时刻的采样值\n",
    "- 公式：\n",
    "  $q(x_t|x_0) = N(x_t; \\sqrt{\\bar{a_t}}x_0, \\sqrt{1-\\bar{a_t}}I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch:0/10:   0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9631\n",
      "Loss: 0.6060\n",
      "Loss: 0.4160\n",
      "Loss: 0.2870\n",
      "Loss: 0.2865\n",
      "Loss: 0.2225\n",
      "Loss: 0.2206\n",
      "Loss: 0.2105\n",
      "Loss: 0.1761\n",
      "Loss: 0.1709\n",
      "Loss: 0.1695\n",
      "Loss: 0.1703\n",
      "Loss: 0.1652\n",
      "Loss: 0.1288\n",
      "Loss: 0.1447\n",
      "Loss: 0.1578\n",
      "Loss: 0.1415\n",
      "Loss: 0.1338\n",
      "Loss: 0.1393\n",
      "Loss: 0.1382\n",
      "Loss: 0.1435\n",
      "Loss: 0.1563\n",
      "Loss: 0.1377\n",
      "Loss: 0.1143\n",
      "Loss: 0.1133\n",
      "Loss: 0.1226\n",
      "Loss: 0.1327\n",
      "Loss: 0.1232\n",
      "Loss: 0.0986\n",
      "Loss: 0.1382\n",
      "Loss: 0.0940\n",
      "Loss: 0.1255\n",
      "Loss: 0.1303\n",
      "Loss: 0.1105\n",
      "Loss: 0.1254\n",
      "Loss: 0.1054\n",
      "Loss: 0.0987\n",
      "Loss: 0.0920\n",
      "Loss: 0.1086\n",
      "Loss: 0.0951\n",
      "Loss: 0.1173\n",
      "Loss: 0.0886\n",
      "Loss: 0.1000\n",
      "Loss: 0.1095\n",
      "Loss: 0.0930\n",
      "Loss: 0.0964\n",
      "Loss: 0.1010\n",
      "Loss: 0.0880\n",
      "Loss: 0.0981\n",
      "Loss: 0.0938\n",
      "Loss: 0.0828\n",
      "Loss: 0.0859\n",
      "Loss: 0.0945\n",
      "Loss: 0.0900\n",
      "Loss: 0.0670\n",
      "Loss: 0.0882\n",
      "Loss: 0.0898\n",
      "Loss: 0.0868\n",
      "Loss: 0.0758\n",
      "Loss: 0.0712\n",
      "Loss: 0.0695\n",
      "Loss: 0.0795\n",
      "Loss: 0.0684\n",
      "Loss: 0.0681\n",
      "Loss: 0.0869\n",
      "Loss: 0.0808\n",
      "Loss: 0.0826\n",
      "Loss: 0.0643\n",
      "Loss: 0.0706\n",
      "Loss: 0.0906\n",
      "Loss: 0.0853\n",
      "Loss: 0.0654\n",
      "Loss: 0.0610\n",
      "Loss: 0.0746\n",
      "Loss: 0.0854\n",
      "Loss: 0.0711\n",
      "Loss: 0.0611\n",
      "Loss: 0.0602\n",
      "Loss: 0.0730\n",
      "Loss: 0.0706\n",
      "Loss: 0.0740\n",
      "Loss: 0.0530\n",
      "Loss: 0.0524\n",
      "Loss: 0.0579\n",
      "Loss: 0.0732\n",
      "Loss: 0.0806\n",
      "Loss: 0.0625\n",
      "Loss: 0.0836\n",
      "Loss: 0.0441\n",
      "Loss: 0.0572\n",
      "Loss: 0.0616\n",
      "Loss: 0.0785\n",
      "Loss: 0.0566\n",
      "Loss: 0.0562\n",
      "Loss: 0.0793\n",
      "Loss: 0.0579\n",
      "Loss: 0.0562\n",
      "Loss: 0.0689\n",
      "Loss: 0.0618\n",
      "Loss: 0.0741\n",
      "Loss: 0.0513\n",
      "Loss: 0.0591\n",
      "Loss: 0.0609\n",
      "Loss: 0.0521\n",
      "Loss: 0.0583\n",
      "Loss: 0.0612\n",
      "Loss: 0.0533\n",
      "Loss: 0.0565\n",
      "Loss: 0.0533\n",
      "Loss: 0.0588\n",
      "Loss: 0.0560\n",
      "Loss: 0.0546\n",
      "Loss: 0.0473\n",
      "Loss: 0.0515\n",
      "Loss: 0.0445\n",
      "Loss: 0.0585\n",
      "Loss: 0.0525\n",
      "Loss: 0.0445\n",
      "Loss: 0.0505\n",
      "Loss: 0.0611\n",
      "Loss: 0.0624\n",
      "Loss: 0.0584\n",
      "Loss: 0.0481\n",
      "Loss: 0.0445\n",
      "Loss: 0.0544\n",
      "Loss: 0.0492\n",
      "Loss: 0.0418\n",
      "Loss: 0.0622\n",
      "Loss: 0.0671\n",
      "Loss: 0.0530\n",
      "Loss: 0.0499\n",
      "Loss: 0.0414\n",
      "Loss: 0.0478\n",
      "Loss: 0.0505\n",
      "Loss: 0.0481\n",
      "Loss: 0.0550\n",
      "Loss: 0.0499\n",
      "Loss: 0.0378\n",
      "Loss: 0.0499\n",
      "Loss: 0.0447\n",
      "Loss: 0.0594\n",
      "Loss: 0.0440\n",
      "Loss: 0.0402\n",
      "Loss: 0.0551\n",
      "Loss: 0.0467\n",
      "Loss: 0.0414\n",
      "Loss: 0.0595\n",
      "Loss: 0.0374\n",
      "Loss: 0.0356\n",
      "Loss: 0.0367\n",
      "Loss: 0.0352\n",
      "Loss: 0.0514\n",
      "Loss: 0.0446\n",
      "Loss: 0.0587\n",
      "Loss: 0.0430\n",
      "Loss: 0.0437\n",
      "Loss: 0.0492\n",
      "Loss: 0.0573\n",
      "Loss: 0.0432\n",
      "Loss: 0.0403\n",
      "Loss: 0.0468\n",
      "Loss: 0.0415\n",
      "Loss: 0.0410\n",
      "Loss: 0.0526\n",
      "Loss: 0.0441\n",
      "Loss: 0.0421\n",
      "Loss: 0.0575\n",
      "Loss: 0.0486\n",
      "Loss: 0.0469\n",
      "Loss: 0.0449\n",
      "Loss: 0.0332\n",
      "Loss: 0.0458\n",
      "Loss: 0.0404\n",
      "Loss: 0.0381\n",
      "Loss: 0.0481\n",
      "Loss: 0.0537\n",
      "Loss: 0.0531\n",
      "Loss: 0.0467\n",
      "Loss: 0.0437\n",
      "Loss: 0.0480\n",
      "Loss: 0.0527\n",
      "Loss: 0.0588\n",
      "Loss: 0.0434\n",
      "Loss: 0.0524\n",
      "Loss: 0.0362\n",
      "Loss: 0.0429\n",
      "Loss: 0.0364\n",
      "Loss: 0.0389\n",
      "Loss: 0.0343\n",
      "Loss: 0.0443\n",
      "Loss: 0.0430\n",
      "Loss: 0.0447\n",
      "Loss: 0.0399\n",
      "Loss: 0.0359\n",
      "Loss: 0.0440\n",
      "Loss: 0.0379\n",
      "Loss: 0.0346\n",
      "Loss: 0.0407\n",
      "Loss: 0.0435\n",
      "Loss: 0.0423\n",
      "Loss: 0.0447\n",
      "Loss: 0.0410\n",
      "Loss: 0.0466\n",
      "Loss: 0.0339\n",
      "Loss: 0.0474\n",
      "Loss: 0.0316\n",
      "Loss: 0.0446\n",
      "Loss: 0.0414\n",
      "Loss: 0.0489\n",
      "Loss: 0.0361\n",
      "Loss: 0.0280\n",
      "Loss: 0.0371\n",
      "Loss: 0.0360\n",
      "Loss: 0.0388\n",
      "Loss: 0.0337\n",
      "Loss: 0.0324\n",
      "Loss: 0.0421\n",
      "Loss: 0.0430\n",
      "Loss: 0.0353\n",
      "Loss: 0.0432\n",
      "Loss: 0.0281\n",
      "Loss: 0.0468\n",
      "Loss: 0.0542\n",
      "Loss: 0.0360\n",
      "Loss: 0.0409\n",
      "Loss: 0.0403\n",
      "Loss: 0.0367\n",
      "Loss: 0.0533\n",
      "Loss: 0.0293\n",
      "Loss: 0.0342\n",
      "Loss: 0.0477\n",
      "Loss: 0.0337\n",
      "Loss: 0.0354\n",
      "Loss: 0.0383\n",
      "Loss: 0.0337\n",
      "Loss: 0.0331\n",
      "Loss: 0.0386\n",
      "Loss: 0.0364\n",
      "Loss: 0.0347\n",
      "Loss: 0.0477\n",
      "Loss: 0.0336\n",
      "Loss: 0.0371\n",
      "Loss: 0.0327\n",
      "Loss: 0.0378\n",
      "Loss: 0.0390\n",
      "Loss: 0.0382\n",
      "Loss: 0.0383\n",
      "Loss: 0.0336\n",
      "Loss: 0.0371\n",
      "Loss: 0.0344\n",
      "Loss: 0.0352\n",
      "Loss: 0.0326\n",
      "Loss: 0.0347\n",
      "Loss: 0.0458\n",
      "Loss: 0.0380\n",
      "Loss: 0.0344\n",
      "Loss: 0.0274\n",
      "Loss: 0.0412\n",
      "Loss: 0.0393\n",
      "Loss: 0.0364\n",
      "Loss: 0.0368\n",
      "Loss: 0.0354\n",
      "Loss: 0.0259\n",
      "Loss: 0.0303\n",
      "Loss: 0.0426\n",
      "Loss: 0.0395\n",
      "Loss: 0.0301\n",
      "Loss: 0.0414\n",
      "Loss: 0.0407\n",
      "Loss: 0.0364\n",
      "Loss: 0.0360\n",
      "Loss: 0.0361\n",
      "Loss: 0.0370\n",
      "Loss: 0.0350\n",
      "Loss: 0.0363\n",
      "Loss: 0.0310\n",
      "Loss: 0.0333\n",
      "Loss: 0.0442\n",
      "Loss: 0.0344\n",
      "Loss: 0.0342\n",
      "Loss: 0.0327\n",
      "Loss: 0.0274\n",
      "Loss: 0.0358\n",
      "Loss: 0.0327\n",
      "Loss: 0.0335\n",
      "Loss: 0.0416\n",
      "Loss: 0.0390\n",
      "Loss: 0.0365\n",
      "Loss: 0.0352\n",
      "Loss: 0.0380\n",
      "Loss: 0.0452\n",
      "Loss: 0.0295\n",
      "Loss: 0.0311\n",
      "Loss: 0.0321\n",
      "Loss: 0.0373\n",
      "Loss: 0.0367\n",
      "Loss: 0.0377\n",
      "Loss: 0.0463\n",
      "Loss: 0.0417\n",
      "Loss: 0.0314\n",
      "Loss: 0.0349\n",
      "Loss: 0.0357\n",
      "Loss: 0.0343\n",
      "Loss: 0.0321\n",
      "Loss: 0.0351\n",
      "Loss: 0.0377\n",
      "Loss: 0.0300\n",
      "Loss: 0.0377\n",
      "Loss: 0.0287\n",
      "Loss: 0.0383\n",
      "Loss: 0.0401\n",
      "Loss: 0.0308\n",
      "Loss: 0.0457\n",
      "Loss: 0.0322\n",
      "Loss: 0.0333\n",
      "Loss: 0.0271\n",
      "Loss: 0.0304\n",
      "Loss: 0.0330\n",
      "Loss: 0.0349\n",
      "Loss: 0.0336\n",
      "Loss: 0.0442\n",
      "Loss: 0.0303\n",
      "Loss: 0.0253\n",
      "Loss: 0.0422\n",
      "Loss: 0.0276\n",
      "Loss: 0.0313\n",
      "Loss: 0.0322\n",
      "Loss: 0.0365\n",
      "Loss: 0.0290\n",
      "Loss: 0.0397\n",
      "Loss: 0.0284\n",
      "Loss: 0.0265\n",
      "Loss: 0.0268\n",
      "Loss: 0.0217\n",
      "Loss: 0.0374\n",
      "Loss: 0.0342\n",
      "Loss: 0.0415\n",
      "Loss: 0.0339\n",
      "Loss: 0.0321\n",
      "Loss: 0.0337\n",
      "Loss: 0.0319\n",
      "Loss: 0.0316\n",
      "Loss: 0.0389\n",
      "Loss: 0.0286\n",
      "Loss: 0.0301\n",
      "Loss: 0.0349\n",
      "Loss: 0.0367\n",
      "Loss: 0.0285\n",
      "Loss: 0.0373\n",
      "Loss: 0.0461\n",
      "Loss: 0.0356\n",
      "Loss: 0.0284\n",
      "Loss: 0.0387\n",
      "Loss: 0.0438\n",
      "Loss: 0.0375\n",
      "Loss: 0.0363\n",
      "Loss: 0.0323\n",
      "Loss: 0.0368\n",
      "Loss: 0.0324\n",
      "Loss: 0.0356\n",
      "Loss: 0.0368\n",
      "Loss: 0.0360\n",
      "Loss: 0.0438\n",
      "Loss: 0.0378\n",
      "Loss: 0.0361\n",
      "Loss: 0.0450\n",
      "Loss: 0.0318\n",
      "Loss: 0.0432\n",
      "Loss: 0.0306\n",
      "Loss: 0.0318\n",
      "Loss: 0.0321\n",
      "Loss: 0.0401\n",
      "Loss: 0.0331\n",
      "Loss: 0.0314\n",
      "Loss: 0.0245\n",
      "Loss: 0.0362\n",
      "Loss: 0.0284\n",
      "Loss: 0.0320\n",
      "Loss: 0.0344\n",
      "Loss: 0.0410\n",
      "Loss: 0.0364\n",
      "Loss: 0.0330\n",
      "Loss: 0.0338\n",
      "Loss: 0.0342\n",
      "Loss: 0.0294\n",
      "Loss: 0.0302\n",
      "Loss: 0.0366\n",
      "Loss: 0.0289\n",
      "Loss: 0.0304\n",
      "Loss: 0.0273\n",
      "Loss: 0.0304\n",
      "Loss: 0.0349\n",
      "Loss: 0.0320\n",
      "Loss: 0.0231\n",
      "Loss: 0.0346\n",
      "Loss: 0.0284\n",
      "Loss: 0.0421\n",
      "Loss: 0.0369\n",
      "Loss: 0.0370\n",
      "Loss: 0.0287\n",
      "Loss: 0.0361\n",
      "Loss: 0.0311\n",
      "Loss: 0.0334\n",
      "Loss: 0.0301\n",
      "Loss: 0.0485\n",
      "Loss: 0.0348\n",
      "Loss: 0.0280\n",
      "Loss: 0.0348\n",
      "Loss: 0.0319\n",
      "Loss: 0.0313\n",
      "Loss: 0.0347\n",
      "Loss: 0.0317\n",
      "Loss: 0.0314\n",
      "Loss: 0.0301\n",
      "Loss: 0.0375\n",
      "Loss: 0.0340\n",
      "Loss: 0.0343\n",
      "Loss: 0.0254\n",
      "Loss: 0.0305\n",
      "Loss: 0.0284\n",
      "Loss: 0.0365\n",
      "Loss: 0.0251\n",
      "Loss: 0.0324\n",
      "Loss: 0.0283\n",
      "Loss: 0.0286\n",
      "Loss: 0.0412\n",
      "Loss: 0.0277\n",
      "Loss: 0.0286\n",
      "Loss: 0.0291\n",
      "Loss: 0.0308\n",
      "Loss: 0.0327\n",
      "Loss: 0.0280\n",
      "Loss: 0.0304\n",
      "Loss: 0.0313\n",
      "Loss: 0.0331\n",
      "Loss: 0.0451\n",
      "Loss: 0.0285\n",
      "Loss: 0.0314\n",
      "Loss: 0.0399\n",
      "Loss: 0.0300\n",
      "Loss: 0.0275\n",
      "Loss: 0.0284\n",
      "Loss: 0.0345\n",
      "Loss: 0.0280\n",
      "Loss: 0.0294\n",
      "Loss: 0.0278\n",
      "Loss: 0.0298\n",
      "Loss: 0.0349\n",
      "Loss: 0.0262\n",
      "Loss: 0.0357\n",
      "Loss: 0.0281\n",
      "Loss: 0.0364\n",
      "Loss: 0.0332\n",
      "Loss: 0.0310\n",
      "Loss: 0.0382\n",
      "Loss: 0.0376\n",
      "Loss: 0.0292\n",
      "Loss: 0.0296\n",
      "Loss: 0.0320\n",
      "Loss: 0.0355\n",
      "Loss: 0.0246\n",
      "Loss: 0.0281\n",
      "Loss: 0.0313\n",
      "Loss: 0.0338\n",
      "Loss: 0.0305\n",
      "Loss: 0.0272\n",
      "Loss: 0.0308\n",
      "Loss: 0.0364\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (96) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6315/1968069483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                         device=device)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mX_t_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReal_noise_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_diffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_0_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# the model are asked to predict the noise added in diffusing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6315/2125565049.py\u001b[0m in \u001b[0;36mforward_diffusion\u001b[0;34m(DS, X_0, batch_timestep)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_timestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                dim=0) # 维度 [bz]\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sqrt_alpha_cumulative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     batch_sqrt_one_minus_alpha_cumulative = torch.gather(input=DS.sqrt_one_minus_alpha_cumulative, \n\u001b[1;32m     16\u001b[0m                                                          \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_timestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (96) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    with tqdm(total=len(train_loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch:{epoch}/{epochs}\")\n",
    "    \n",
    "    for X_0_batch, _ in train_loader:\n",
    "        X_0_batch = X_0_batch.to(device)\n",
    "        # Assign a batch of timesteps to each X0 sample\n",
    "        batch_timesteps = torch.randint(low=1, \n",
    "                                        high=num_diffusion_timestep, \n",
    "                                        size=(X_0_batch.shape[0],), \n",
    "                                        device=device)\n",
    "        \n",
    "        X_t_batch, Real_noise_batch = forward_diffusion(DS=DS, X_0=X_0_batch, batch_timestep=batch_timesteps)\n",
    "        # the model are asked to predict the noise added in diffusing\n",
    "        \n",
    "        pred_noise = model(X_t_batch, batch_timesteps)\n",
    "        loss = loss_fn(Real_noise_batch, pred_noise)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        tq.set_postfix_str(s=f\"Loss: {loss.item():.4f}\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "    tq.set_postfix_str(s=f\"Epoch Loss: {total_loss/len(train_loader):.4f}\") \n",
    "    print(f\"Epoch Loss: {total_loss/len(train_loader):.4f}\")   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逆扩散过程（Sampling）\n",
    "$\\mathbf{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t, t\\right)\\right)+\\sigma_t \\mathbf{z}$\n",
    "\n",
    "$\\sigma_t = \\frac{1-\\bar{\\alpha_{t-1}}}{1-\\bar{\\alpha}_t} * {beta_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def denoising_diffusion(model, DS:diffusion_setting, X_t, timestep, start_at_T):\n",
    "    eps = torch.randn_like(X_t)  if not start_at_T else torch.zeros_like(X_t)\n",
    "    predicted_noise = model(X_t, timestep)\n",
    "    \n",
    "    beta_t = torch.gather(input=DS.beta, index=timestep, dim=0).reshape(-1, 1, 1, 1)\n",
    "    one_by_sqrt_alpha_t = torch.gather(input=DS.one_by_sqrt_alpha_s, index=timestep, dim=0).reshape(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alpha_cumulative_t = torch.gather(input=DS.sqrt_one_minus_alpha_cumulative, index=timestep, dim=0).reshape(-1, 1, 1, 1)\n",
    "    \n",
    "    mean = one_by_sqrt_alpha_t * (X_t - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "    # todo here, the authors take the sqrt(beta_t) from diffusing process\n",
    "    # instead of mathematical results of [1-alpha_cumulative(t-1)] * beta_t / [1-alpha_cumulative(t)]\n",
    "    var = torch.gather(input=DS.sqrt_beta_s, index=timestep, dim=0).reshape(-1, 1, 1, 1)\n",
    "    X_t_minus_1 = mean + var * eps\n",
    "    return X_t_minus_1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling ::   0%|          | 0/999 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 7.70 GiB already allocated; 28.12 MiB free; 7.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6315/724489529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                               \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                               \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                               start_at_T=True if time_step == 1 else False)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6315/3947861929.py\u001b[0m in \u001b[0;36mdenoising_diffusion\u001b[0;34m(model, DS, X_t, timestep, start_at_T)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdenoising_diffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdiffusion_setting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_at_T\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbeta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6315/537829620.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6315/537829620.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# group 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormlize_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv_project/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1899\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 7.70 GiB already allocated; 28.12 MiB free; 7.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_images = 5\n",
    "img_shape = (1, 32, 32)\n",
    "x_T = torch.randn((num_images, *img_shape), device=device)  # [num_images, C, H, W]\n",
    "x_t = x_T  # the first X_t\n",
    "for time_step in tqdm(iterable=reversed(range(1, num_diffusion_timestep)), \n",
    "                      total=num_diffusion_timestep-1, dynamic_ncols=False,\n",
    "                      desc=\"Sampling :: \", position=0):\n",
    "    time_step_batch = torch.ones(num_images, \n",
    "                                 dtype=torch.long, \n",
    "                                 device=device) * time_step\n",
    "    x_t = denoising_diffusion(model, \n",
    "                              DS, \n",
    "                              x_t, \n",
    "                              timestep=time_step_batch, \n",
    "                              start_at_T=True if time_step == 1 else False)\n",
    "    print(x_t.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('cv_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4a9c97a8b021f57fd72049f636b97cda4a73e3b574c4423a7c228fa056b5e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
