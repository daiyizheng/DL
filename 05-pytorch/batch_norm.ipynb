{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNorm\n",
    "\n",
    "批量归一化的本质目的：学习底部层的时候避免顶部层变化                     \n",
    "所以只有在使用深层的网络结构的时候才会使用BN，浅层的如mlp效果不明显                                      \n",
    "优点：不会改变模型的精度，但是可以加速模型收敛                                             \n",
    "首先我们先明确引起变化的原因，是每一层的方差和均值的分布不同。所以我们要固定所有层的分布，尽量符合同一个分布。              \n",
    "批量归一化的作用原理：固定小批量里的均值和方差     \n",
    "\n",
    "\n",
    "优点：不会改变模型的精度，但是可以加速模型收敛    \n",
    "\n",
    "$\\mu_B=\\frac{1}{|B|} \\sum_{i \\in B} x_i \\text { and } \\sigma_B^2=\\frac{1}{|B|} \\sum_{i \\in B}\\left(x_i-\\mu_B\\right)^2+\\epsilon$\n",
    "\n",
    "\n",
    "1. 首先求出均值和方差\n",
    "2. 再做额外的调整，输入的是实际数据$x_i$，输出是归一化后的$x_{i+1}$，其中$gama$和$beta$是可以学习的参数\n",
    "\n",
    "\n",
    "$y=\\frac{x-\\mathrm{E}[x]}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}} * \\gamma+\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X:torch.Tensor, \n",
    "               gamma:torch.Tensor, \n",
    "               beta:torch.Tensor, \n",
    "               moving_mean:torch.Tensor, \n",
    "               moving_var:torch.Tensor, \n",
    "               eps, \n",
    "               momentum):\n",
    "    if not torch.is_grad_enabled():\n",
    "        # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式\n",
    "        x_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    \n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        #判断是全连接层还是卷积层，2代表全连接层，样本数和特征数；4代表卷积层，批量数，通道数，高宽\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(dim=0, keepdim=True) # []\n",
    "            var = ((X-mean)**2).mean(dim=0, keepdim=True)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
    "            # 这里我们需要保持X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            #1*n*高*宽\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # 训练模式下，用当前的均值和方差做标准化\n",
    "        x_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * x_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, channels, num_dims) -> None:\n",
    "        # num_features：完全连接层的输出数量或卷积层的输出通道数。\n",
    "        # num_dims：2表示完全连接层，4表示卷积层\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_dims==2:\n",
    "            shape = (1, channels)\n",
    "        else:\n",
    "            shape = (1, channels, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "        \n",
    "    def forward(self, X:torch.Tensor):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var\n",
    "        # 复制到X所在显存上\n",
    "        if X.device != self.moving_mean.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(X, \n",
    "                                                          self.gamma, \n",
    "                                                          self.beta, \n",
    "                                                          self.moving_mean,\n",
    "                                                          self.moving_var,\n",
    "                                                          eps=1e-5, \n",
    "                                                          momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BatchNorm(3, num_dims=4)\n",
    "model2 = nn.BatchNorm2d(3)\n",
    "input_ = torch.randn(size=(5, 3, 24, 24))\n",
    "res = model(input_)\n",
    "res2 = model2(input_)\n",
    "p = torch.allclose(res, res2)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('cv_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4a9c97a8b021f57fd72049f636b97cda4a73e3b574c4423a7c228fa056b5e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
