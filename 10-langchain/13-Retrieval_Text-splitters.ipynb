{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters\n",
    "一旦加载了文档，您通常会想要转换它们以更好地适应您的应用程序。最简单的例子是，您可能希望将长文档分成更小的块，以便适合模型的上下文窗口。LangChain有许多内置的文档转换器，可以方便地拆分、组合、过滤和操作文档。\n",
    "\n",
    "当您想要处理长文本时，有必要将该文本分割成块。虽然这听起来很简单，但这里有很多潜在的复杂性。理想情况下，您希望将语义相关的文本片段保持在一起。“语义相关”的含义可能取决于文本的类型。本笔记本展示了实现这一目标的几种方法。\n",
    "\n",
    "在高层次上，文本分割器的工作方式如下\n",
    "1. 将文本分成语义上有意义的小块(通常是句子)。\n",
    "2. 开始将这些小块组合成一个更大的块，直到达到一定的大小(由某个函数测量)。\n",
    "3. 一旦你达到这个大小，让这个块成为它自己的文本块，然后开始创建一个有一些重叠的新文本块(以保持块之间的上下文)。\n",
    "\n",
    "这意味着有两个不同的轴，您可以沿着它自定义文本分割器\n",
    "1. 文本是如何分割的\n",
    "2. 如何测量块大小\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.globals import set_debug\n",
    "import os\n",
    "load_dotenv(find_dotenv())\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Text Splitters\n",
    "LangChain提供了许多不同类型的文本分割器。这些都在langchain-text-splitter包中。下面的表格列出了所有这些，以及一些特征      \n",
    "Name:文本分割器的名称\n",
    "Splits On:这个文本分割器如何分割文本\n",
    "Adds Metadata:该文本分割器是否添加关于每个块来自何处的元数据。\n",
    "Description:拆分器的描述，包括何时使用它的建议。\n",
    "\n",
    "表格\n",
    "| Name | Splits On | Adds Metadata | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| Recursive | 用户自定义字符的列表 |  | 递归分割文本。递归地分割文本的目的是试图保持相关的文本片段彼此相邻。这是开始分割文本的推荐方法。 |\n",
    "| HTML | HTML特定字符 | 是 | 基于特定于html的字符拆分文本。值得注意的是，这增加了有关数据块来自何处的相关信息(基于HTML)。 |\n",
    "| Markdown | 减记特定字符 | 是 | 根据特定标记字符拆分文本。值得注意的是，这添加了关于该块来自何处的相关信息(基于Markdown)。 |\n",
    "| Code | 代码(Python, JS)特定字符 |  | 基于特定于编码语言的字符拆分文本。有15种不同的语言可供选择。 |\n",
    "| Token | Tokens |  | 分割标记上的文本。有几种不同的方法来衡量代币。 |\n",
    "| Character | 用户定义的字符 |  | 根据用户定义的字符拆分文本。一个更简单的方法。 |\n",
    "| [实验]语义分块器 | 句子 |  | 首先拆分句子。然后，如果它们在语义上足够相似，就将它们相邻地组合在一起。取自格雷格·卡姆拉特 |\n",
    "| AI21语义文本分配器 | Semantics | 是 | 识别形成连贯文本的不同主题，并沿着这些主题进行拆分。 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算文本分配器\n",
    "您可以使用Greg Kamradt创建的Chunkviz实用程序来评估文本分割器。Chunkviz是一个很好的工具，可以可视化你的文本分配器是如何工作的。它将向您展示文本是如何分割的，并帮助您调整分割参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Document Transforms\n",
    "文本分割只是您可能希望在将文档传递给LLM之前对其进行转换的一个示例。转到集成，获取关于与第三方工具集成的内置文档转换器的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by HTML header\n",
    " `MarkdownHeaderTextSplitter`，`HTMLHeaderTextSplitter`是一个结构感知的分块器，在元素级别拆分文本，并为每个头添加与任何给定块相关的元数据。它可以一个元素一个元素地返回块，或者将具有相同元数据的元素组合起来，其目的是(a)在语义上保持相关文本的分组(或多或少)，以及(b)保留在文档结构中编码的上下文丰富的信息。它可以与其他文本分割器一起使用，作为分块管道的一部分。\n",
    "\n",
    "### Usage examples\n",
    "%pip install -qU langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Foo'),\n",
       " Document(page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some intro text about Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}),\n",
       " Document(page_content='Some text about the first subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}),\n",
       " Document(page_content='Some text about the second subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}),\n",
       " Document(page_content='Baz', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some text about Baz', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}),\n",
       " Document(page_content='Some concluding text about Foo', metadata={'Header 1': 'Foo'})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We see that Gödel first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (“This sentence is false”) and Berry’s paradox (“The least number not defined by an expression consisting of just fourteen English words”). Gödel then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='means that arithmetic truth and arithmetic provability are not co-extensive — whence the First Incompleteness Theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='This account of Gödel’s discovery was told to Hao Wang very much after the fact; but in Gödel’s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See Gödel 2003a and Gödel 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by Gödel by 1931. But he neither publicized nor published the result; the biases logicians', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to Gödel’s publication of that theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='We now describe the proof of the two theorems, formulating Gödel’s results in Peano arithmetic. Gödel himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following Gödel’s notation.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "splits[80:85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 局限性\n",
    "从一个HTML文档到另一个文档可能会有很多结构上的变化，尽管HTMLHeaderTextSplitter会尝试将所有“相关”标头附加到任何给定的块上，但有时可能会错过某些标题。例如，该算法假定一个信息层次结构，其中标题始终处于节点“上方”相关文本，即先前的兄弟姐妹，祖先及其组合。在以下新闻文章（从本文档的撰写中）中，该文档的结构化使得顶级标题的文本何时标记为“ H1”，在我们期望的文本元素中截然不同它要在“上方” - 因此，我们可以观察到“ H1”元素及其相关文本不会显示在块元数据中（但是，在适用的情况下，我们确实看到“ H2”及其相关文本）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No two El Niño winters are the same, but many have temperature and precipitation trends in common.  \n",
      "Average conditions during an El Niño winter across the continental US.  \n",
      "One of the major reasons is the position of the jet stream, which often shifts south during an El Niño winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  \n",
      "Because the jet stream is essentially a river of air that storms flow through, they c\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "print(html_header_splits[1].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by HTML section\n",
    "HTMLSectionsPlitter上的概念与HTMLHeaderTextSplitter相似，是一个“结构感知”的零件，可将文本分配为元素级别，并为每个给定的块添加每个标题“相关”的元数据。它可以通过元素返回块，或将元素与相同的元数据结合在一起，以及（a）将相关文本（或多或少）的相关文本进行分组（b）保存在文档结构中编码的上下文丰富的信息的目标。它可以与其他文本拆分器一起用作块管道的一部分。在内部，当截面尺寸大于块大小时，它使用递归的Cearsivecharactertextsplitter。它还考虑文本的字体大小，以确定它是否基于确定的字体大小阈值。使用XSLT_Path提供一个绝对的路径来转换HTML，以便它可以基于提供的标签检测部分。默认值是在data_connection/docunnnection_transformers目录中使用converting_to_header.xslt文件。这是为了将HTML转换为更易于检测部分的格式/布局。例如，可以将基于其字体大小的跨度转换为标题标签以检测为部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HTMLSectionSplitter' from 'langchain_text_splitters' (/Users/dyz/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_text_splitters/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLSectionSplitter\n\u001b[1;32m      3\u001b[0m html_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    <!DOCTYPE html>\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    <html>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    </html>\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     29\u001b[0m headers_to_split_on \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 1\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HTMLSectionSplitter' from 'langchain_text_splitters' (/Users/dyz/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_text_splitters/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>Foo</h1>\n",
    "            <p>Some intro text about Foo.</p>\n",
    "            <div>\n",
    "                <h2>Bar main section</h2>\n",
    "                <p>Some intro text about Bar.</p>\n",
    "                <h3>Bar subsection 1</h3>\n",
    "                <p>Some text about the first subtopic of Bar.</p>\n",
    "                <h3>Bar subsection 2</h3>\n",
    "                <p>Some text about the second subtopic of Bar.</p>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h2>Baz</h2>\n",
    "                <p>Some text about Baz</p>\n",
    "            </div>\n",
    "            <br>\n",
    "            <p>Some concluding text about Foo</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "\n",
    "html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)管道到另一个拆分器，与html从html字符串内容加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTMLSectionSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m      3\u001b[0m html_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    <!DOCTYPE html>\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    <html>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    </html>\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     29\u001b[0m headers_to_split_on \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     30\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     31\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     32\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 3\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     33\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader 4\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     34\u001b[0m ]\n\u001b[0;32m---> 36\u001b[0m html_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mHTMLSectionSplitter\u001b[49m(headers_to_split_on\u001b[38;5;241m=\u001b[39mheaders_to_split_on)\n\u001b[1;32m     38\u001b[0m html_header_splits \u001b[38;5;241m=\u001b[39m html_splitter\u001b[38;5;241m.\u001b[39msplit_text(html_string)\n\u001b[1;32m     40\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTMLSectionSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>Foo</h1>\n",
    "            <p>Some intro text about Foo.</p>\n",
    "            <div>\n",
    "                <h2>Bar main section</h2>\n",
    "                <p>Some intro text about Bar.</p>\n",
    "                <h3>Bar subsection 1</h3>\n",
    "                <p>Some text about the first subtopic of Bar.</p>\n",
    "                <h3>Bar subsection 2</h3>\n",
    "                <p>Some text about the second subtopic of Bar.</p>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h2>Baz</h2>\n",
    "                <p>Some text about Baz</p>\n",
    "            </div>\n",
    "            <br>\n",
    "            <p>Some concluding text about Foo</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by character\n",
    "这是最简单的方法。这将基于字符进行分割(默认情况下)，并根据字符数测量块长度。\n",
    "\n",
    "1. 如何分割文本:通过单个字符。\n",
    "2. 如何测量块大小:通过字符数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\n1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\\n1/23/23, 2:59 AM - User 1: How much do you want?\\n1/23/23, 3:00 AM - User 2: Online is at least $100\\n1/23/23, 3:01 AM - User 2: Here is $129\\n1/23/23, 3:01 AM - User 2: <Media omitted>\\n1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\\n1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\\n1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\\n1/23/23, 3:19 AM - User 1: Oh no worries! Bye\\n1/23/23, 3:19 AM - User 2: Bye!\\n1/23/23, 3:22_AM - User 1: And let me know if anything changes'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "state_of_the_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.CharacterTextSplitter at 0x7fa468e01660>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\n1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\\n1/23/23, 2:59 AM - User 1: How much do you want?\\n1/23/23, 3:00 AM - User 2: Online is at least $100\\n1/23/23, 3:01 AM - User 2: Here is $129\\n1/23/23, 3:01 AM - User 2: <Media omitted>\\n1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\\n1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\\n1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\\n1/23/23, 3:19 AM - User 1: Oh no worries! Bye\\n1/23/23, 3:19 AM - User 2: Bye!\\n1/23/23, 3:22_AM - User 1: And let me know if anything changes'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个与文档一起传递元数据的示例，注意它与文档一起被拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\n1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\\n1/23/23, 2:59 AM - User 1: How much do you want?\\n1/23/23, 3:00 AM - User 2: Online is at least $100\\n1/23/23, 3:01 AM - User 2: Here is $129\\n1/23/23, 3:01 AM - User 2: <Media omitted>\\n1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\\n1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\\n1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\\n1/23/23, 3:19 AM - User 1: Oh no worries! Bye\\n1/23/23, 3:19 AM - User 2: Bye!\\n1/23/23, 3:22_AM - User 1: And let me know if anything changes' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "documents = text_splitter.create_documents(\n",
    "    [state_of_the_union, state_of_the_union], metadatas=metadatas\n",
    ")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\n1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\\n1/23/23, 2:59 AM - User 1: How much do you want?\\n1/23/23, 3:00 AM - User 2: Online is at least $100\\n1/23/23, 3:01 AM - User 2: Here is $129\\n1/23/23, 3:01 AM - User 2: <Media omitted>\\n1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\\n1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\\n1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\\n1/23/23, 3:19 AM - User 1: Oh no worries! Bye\\n1/23/23, 3:19 AM - User 2: Bye!\\n1/23/23, 3:22_AM - User 1: And let me know if anything changes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(state_of_the_union)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split code\n",
    "CodeTextSplitter允许您拆分支持多种语言的代码。导入enum Language并指定语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "# Full list of supported languages\n",
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您还可以看到用于给定语言的分隔符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkdownHeaderTextSplitter\n",
    "**Motivation**\n",
    "许多聊天或问答应用程序涉及在嵌入和矢量存储之前对输入文档进行分块处理。\n",
    "\n",
    "Pinecone的这些笔记提供了一些有用的提示\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits\n",
    "type(md_header_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，`MarkdownHeaderTextSplitter`会从输出块的内容中剥离拆分的标题。这可以通过设置`strip_headers = False`来禁用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Foo  \\n## Bar  \\nHi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\n",
       " Document(page_content='### Boo  \\nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\n",
       " Document(page_content='## Baz  \\nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个markdown组中，我们可以应用任何我们想要的文本拆分器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\n",
       " Document(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\n",
       " Document(page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\n",
       " Document(page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\n",
       " Document(page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively split JSON\n",
    "这个json拆分器首先遍历json数据深度，然后构建更小的json块。它试图保持嵌套json对象的完整，但如果需要，将它们分开，以保持最小块大小和最大块大小之间的块。如果值不是一个嵌套的json，而是一个非常大的字符串，那么字符串将不会被分割。如果您需要对块大小进行硬性限制，请考虑在这些块上使用递归文本分割器。拆分列表有一个可选的预处理步骤，首先将它们转换为json (dict)，然后拆分它们。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a large nested json object and will be loaded as a python dict\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"servers\": [{\"url\": \"https://api.smith.langchain.com\", \"description\": \"LangSmith API endpoint.\"}]}\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "# or a list of strings\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the size of the chunks\n",
    "print([len(text) for text in texts][:10])\n",
    "\n",
    "# Reviewing one of these chunks that was bigger we see there is a list object there\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]\n"
     ]
    }
   ],
   "source": [
    "# The json splitter by default does not split lists\n",
    "# the following will preprocess the json and convert list to dict with index:item as key:val pairs\n",
    "texts = splitter.split_text(json_data=json_data, convert_lists=True)\n",
    "# Let's look at the size of the chunks. Now they are all under the max\n",
    "print([len(text) for text in texts][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\", \"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\"}}}}')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also look at the documents\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively split by character\n",
    "对于一般文本，推荐使用此文本拆分器。它由一个字符列表参数化。它试图按顺序分割它们，直到块足够小。默认列表为[\"\\n\\n\"， \"\\n\"， \"\"， \"\"]。这样做的效果是尽可能长时间地将所有段落(然后是句子，然后是单词)放在一起，因为这些段落通常看起来是文本中语义相关性最强的部分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\n1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\\n1/23/23, 2:59 AM - User 1: How much do you want?\\n1/23/23, 3:00 AM - User 2: Online is at least $100\\n1/23/23, 3:01 AM - User 2: Here is $129\\n1/23/23, 3:01 AM - User 2: <Media omitted>\\n1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\\n1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\\n1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\\n1/23/23, 3:19 AM - User 1: Oh no worries! Bye\\n1/23/23, 3:19 AM - User 2: Bye!\\n1/23/23, 3:22_AM - User 1: And let me know if anything changes'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_of_the_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are'\n",
      "page_content='me know if you are interested. Thanks!'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are',\n",
       " 'me know if you are interested. Thanks!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(state_of_the_union)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting text from languages without word boundaries\n",
    "有些书写系统没有单词边界，例如中文、日文和泰文。使用默认分隔符列表[\"\\n\\n\"， \"\\n\"， \"\" \"， \"\"]分割文本可能导致单词在块之间被分割。要将单词放在一起，可以覆盖分隔符列表以包含额外的标点符号\n",
    "\n",
    "- 添加ASCII句号。， Unicode全宽句号。(中文)及表意文字的句号(日文及中文)\n",
    "- 在泰语、缅甸语、高棉语和日语中添加零宽度空格。\n",
    "- 添加ASCII逗号，，Unicode全宽逗号，和Unicode表意符号逗号\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    # Existing args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "根据语义相似度拆分文本。\n",
    "\n",
    "摘自Greg Kamradt的精彩笔记本:5个级别的文本分割所有的功劳都是他的。\n",
    "\n",
    "在高层次上，它分成句子，然后分成3个句子的一组，然后合并嵌入空间中相似的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested.\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakpoints\n",
    "这个分块器的工作原理是决定什么时候把句子分开。这是通过寻找任意两个句子之间嵌入的差异来完成的。当差异超过某个阈值时，它们就会被分割。\n",
    "\n",
    "默认的分割方式是基于百分位数。在这种方法中，计算句子之间的所有差异，然后拆分大于X百分位数的任何差异。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准离差\n",
    "在这种方法中，任何大于X标准差的差异都被分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks! 1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low. 1/23/23, 2:59 AM - User 1: How much do you want? 1/23/23, 3:00 AM - User 2: Online is at least $100\n",
      "1/23/23, 3:01 AM - User 2: Here is $129\n",
      "1/23/23, 3:01 AM - User 2: <Media omitted>\n",
      "1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one! 1/23/23, 3:02 AM - User 1: I thought you were selling the blue one! 1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\n",
      "1/23/23, 3:19 AM - User 1: Oh no worries! Bye\n",
      "1/23/23, 3:19 AM - User 2: Bye! 1/23/23, 3:22_AM - User 1: And let me know if anything changes\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"standard_deviation\"\n",
    ")\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四分点\n",
    "在这种方法中，使用四分位数距离来分割块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks! 1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low. 1/23/23, 2:59 AM - User 1: How much do you want? 1/23/23, 3:00 AM - User 2: Online is at least $100\n",
      "1/23/23, 3:01 AM - User 2: Here is $129\n",
      "1/23/23, 3:01 AM - User 2: <Media omitted>\n",
      "1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one! 1/23/23, 3:02 AM - User 1: I thought you were selling the blue one! 1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\n",
      "1/23/23, 3:19 AM - User 1: Oh no worries! Bye\n",
      "1/23/23, 3:19 AM - User 2: Bye! 1/23/23, 3:22_AM - User 1: And let me know if anything changes\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\"\n",
    ")\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by tokens\n",
    "语言模型有一个令牌限制。您不应超过令牌限制。因此，当您将文本分割成块时，计算标记的数量是一个好主意。有很多标记器。在计算文本中的标记时，应该使用与语言模型中使用的相同的标记器。\n",
    "\n",
    "### tiktoken\n",
    "> tiktoken是由OpenAI创建的快速BPE标记器。\n",
    "\n",
    "我们可以用它来估计使用的令牌。对于OpenAI模型来说，这可能会更准确。\n",
    "- 如何分割文本:通过传入的字符。\n",
    "- 如何测量块大小:通过tiktoken标记器。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.from_tiktoken_encoder()`方法将编码作为参数（例如CL100K_BASE）或`model_name`（例如GPT-4）。所有其他参数（例如`chunk_size`，`chunk_overlap`和`saparters`）用于实例化`parnectextsplitter`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\n",
      "1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\n",
      "1/23/23, 2:59 AM - User 1: How much do you want?\n",
      "1/23/23, 3:00 AM - User 2: Online is at least $100\n",
      "1/23/23, 3:01 AM - User 2: Here is $129\n",
      "1/23/23, 3:01 AM - User 2: <Media omitted>\n",
      "1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\n",
      "1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\n",
      "1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\n",
      "1/23/23, 3:19 AM - User 1: Oh no worries! Bye\n",
      "1/23/23, 3:19 AM - User 2: Bye!\n",
      "1/23/23, 3:22_AM - User 1: And let me know if anything changes\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，如果我们使用`CharacterTextSplitter.from tiktoken`编码器，文本只被`CharacterTextSplitter`分割，而tiktoken标记器用于合并分割。这意味着分割可以大于由tiktoken标记器测量的块大小。我们可以使用`RecursiveCharacterTextSplitter.from tiktoken`编码器来确保分割不大于语言模型允许的令牌块大小，其中每个分割将被递归分割，如果它有更大的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以直接加载一个tiktoken拆分器，这将确保每个拆分都小于块大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些书面语言(如汉语和日语)有编码为2个或更多符号的字符。直接使用`TokenTextSplitter`可以将字符的令牌拆分为两个块，从而导致Unicode字符的畸形。使用`RecursiveCharacterTextSplitter.from tiktoken`编码器或`CharacterTextSplitter.from tiktoken`编码器来确保块包含有效的Unicode字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "> spaCy是一个用于高级自然语言处理的开源软件库，用Python和Cython编程语言编写。\n",
    "\n",
    "NLTK的另一个替代方案是使用space标记器。\n",
    "- 如何分割文本:通过space标记器。\n",
    "- 如何测量块大小:通过字符数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     state_of_the_union \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpacyTextSplitter\n\u001b[0;32m----> 6\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mSpacyTextSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(state_of_the_union)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_text_splitters/spacy.py:27\u001b[0m, in \u001b[0;36mSpacyTextSplitter.__init__\u001b[0;34m(self, separator, pipeline, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the spacy text splitter.\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_make_spacy_pipeline_for_splitting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator \u001b[38;5;241m=\u001b[39m separator\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_text_splitters/spacy.py:53\u001b[0m, in \u001b[0;36m_make_spacy_pipeline_for_splitting\u001b[0;34m(pipeline, max_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m     sentencizer\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     sentencizer \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtagger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     sentencizer\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentencizer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformers\n",
    "本分割器。默认行为是将文本分割成适合您想要使用的句子转换器模型的令牌窗口的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyz/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\n",
    "text = \"Lorem \"\n",
    "count_start_and_stop_tokens = 2\n",
    "text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
    "print(text_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in text to split: 388\n",
      "lorem\n"
     ]
    }
   ],
   "source": [
    "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1\n",
    "\n",
    "# `text_to_split` does not fit in a single chunk\n",
    "text_to_split = text * token_multiplier\n",
    "\n",
    "print(f\"tokens in text to split: {splitter.count_tokens(text=text_to_split)}\")\n",
    "\n",
    "text_chunks = splitter.split_text(text=text_to_split)\n",
    "\n",
    "print(text_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "> 自然语言工具包，或者更常见的NLTK，是一套用Python编程语言编写的用于英语的符号和统计自然语言处理(NLP)的库和程序。\n",
    "\n",
    "我们可以使用NLTK来基于NLTK标记器进行分割，而不仅仅是分割。\n",
    "- 文本是如何分割的:通过NLTK标记器\n",
    "- 如何测量块大小:通过字符数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi!\n",
      "\n",
      "Im interested in your bag.\n",
      "\n",
      "Im offering $50.\n",
      "\n",
      "Let me know if you are interested.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "1/22/23, 8:24 PM - User 2: Goodmorning!\n",
      "\n",
      "$50 is too low.\n",
      "\n",
      "1/23/23, 2:59 AM - User 1: How much do you want?\n",
      "\n",
      "1/23/23, 3:00 AM - User 2: Online is at least $100\n",
      "1/23/23, 3:01 AM - User 2: Here is $129\n",
      "1/23/23, 3:01 AM - User 2: <Media omitted>\n",
      "1/23/23, 3:01 AM - User 1: Im not interested in this bag.\n",
      "\n",
      "Im interested in the blue one!\n",
      "\n",
      "1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\n",
      "\n",
      "1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\n",
      "1/23/23, 3:19 AM - User 1: Oh no worries!\n",
      "\n",
      "Bye\n",
      "1/23/23, 3:19 AM - User 2: Bye!\n",
      "\n",
      "1/23/23, 3:22_AM - User 1: And let me know if anything changes\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPY\n",
    "> Konlpy：Python中的韩国NLP是朝鲜语的自然语言处理（NLP）的Python包装。\n",
    "\n",
    "令牌拆分涉及将文本分割为较小，更易于管理的单元，称为令牌。这些令牌通常是单词，短语，符号或其他有意义的元素，对于进一步的处理和分析至关重要。在英语等语言中，令牌分裂通常涉及通过空间和标点符号分开单词。令牌分裂的有效性很大程度上取决于令牌者对语言结构的理解，从而确保产生有意义的令牌。由于为英语设计的象征器没有能力理解其他语言的独特语义结构，例如韩文，因此无法有效地用于韩国语言处理。\n",
    "#### 使用KoNLPy的Kkma分析器进行韩文令牌分割\n",
    "对于韩语文本，KoNLPY包括一个名为Kkma(韩语知识语素分析器)的词形分析器。Kkma提供了韩语文本的详细形态学分析。它将句子分解成单词，单词分解成各自的语素，为每个符号识别词性。它可以将文本块分割成单独的句子，这在处理长文本时特别有用。\n",
    "\n",
    "#### Usage Considerations\n",
    "虽然Kkma以其详细的分析而闻名，但重要的是要注意这种精度可能会影响处理速度。因此，Kkma最适合于分析深度优先于快速文本处理的应用程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22 /23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks! 1/22 /23, 8:24 PM - User 2: Goodmorning! $50 is too low. 1/23 /23, 2:59 AM - User 1: How much do you want? 1/23 /23, 3:00 AM - User 2: Online is at least $100 1/23 /23, 3:01 AM - User 2: Here is $129 1/23 /23, 3:01 AM - User 2: <Media omitted> 1/23 /23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one! 1/23 /23, 3:02 AM - User 1: I thought you were selling the blue one! 1/23 /23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale 1/23 /23, 3:19 AM - User 1: Oh no worries! Bye 1/23 /23, 3:19 AM - User 2: Bye! 1/23 /23, 3:22 _AM - User 1: And let me know if anything changes\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    korean_document = f.read()\n",
    "    \n",
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "text_splitter = KonlpyTextSplitter()\n",
    "texts = text_splitter.split_text(korean_document)\n",
    "# The sentences are split with \"\\n\\n\" characters.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face tokenizer\n",
    "拥抱的脸有许多令牌。\n",
    "\n",
    "我们使用`gpt2tokenizerfast`的拥抱脸令牌来计算令牌中的文本长度。\n",
    "- 如何分割文本:通过传入的字符。\n",
    "- 如何测量块大小:通过拥抱脸标记器计算的标记数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22/23, 6:30 PM - User 1: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\n",
      "1/22/23, 8:24 PM - User 2: Goodmorning! $50 is too low.\n",
      "1/23/23, 2:59 AM - User 1: How much do you want?\n",
      "1/23/23, 3:00 AM - User 2: Online is at least $100\n",
      "1/23/23, 3:01 AM - User 2: Here is $129\n",
      "1/23/23, 3:01 AM - User 2: <Media omitted>\n",
      "1/23/23, 3:01 AM - User 1: Im not interested in this bag. Im interested in the blue one!\n",
      "1/23/23, 3:02 AM - User 1: I thought you were selling the blue one!\n",
      "1/23/23, 3:18 AM - User 2: No Im sorry it was my mistake, the blue one is not for sale\n",
      "1/23/23, 3:19 AM - User 1: Oh no worries! Bye\n",
      "1/23/23, 3:19 AM - User 2: Bye!\n",
      "1/23/23, 3:22_AM - User 1: And let me know if anything changes\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open(\"data/whatsapp_chat.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
