{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "LangChainæä¾›äº†ä¸€ä¸ªå›è°ƒç³»ç»Ÿï¼Œå…è®¸æ‚¨æŒ‚é’©åˆ°LLMåº”ç”¨ç¨‹åºçš„å„ä¸ªé˜¶æ®µã€‚è¿™å¯¹äºæ—¥å¿—è®°å½•ã€ç›‘è§†ã€æµå¤„ç†å’Œå…¶ä»–ä»»åŠ¡éå¸¸æœ‰ç”¨\n",
    "\n",
    "æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨æ•´ä¸ªAPIä¸­å¯ç”¨çš„callbackå‚æ•°æ¥è®¢é˜…è¿™äº›äº‹ä»¶ã€‚è¯¥å‚æ•°æ˜¯å¤„ç†ç¨‹åºå¯¹è±¡çš„åˆ—è¡¨ï¼Œè¿™äº›å¯¹è±¡å°†å®ç°ä¸‹é¢æ›´è¯¦ç»†æè¿°çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ–¹æ³•ã€‚\n",
    "\n",
    "## Callback handlers\n",
    "`CallbackHandlers`æ˜¯å®ç°`CallbackHandler`æ¥å£çš„å¯¹è±¡ï¼Œè¯¥æ¥å£ä¸ºæ¯ä¸ªå¯ä»¥è®¢é˜…çš„äº‹ä»¶æä¾›ä¸€ä¸ªæ–¹æ³•ã€‚å½“äº‹ä»¶è¢«è§¦å‘æ—¶ï¼Œ`CallbackManager`å°†åœ¨æ¯ä¸ªå¤„ç†ç¨‹åºä¸Šè°ƒç”¨é€‚å½“çš„æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.agents import AgentFinish, AgentAction\n",
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "Langchainæä¾›äº†ä¸€äº›å†…ç½®å¤„ç†ç¨‹åºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å¤„ç†ç¨‹åºæ¥å¼€å§‹ã€‚è¿™äº›å¯åœ¨langchain_core/å›è°ƒæ¨¡å—ä¸­å¯ç”¨ã€‚æœ€åŸºæœ¬çš„å¤„ç†ç¨‹åºæ˜¯`StdOutCallbackHandler`ï¼Œå®ƒåªæ˜¯å°†æ‰€æœ‰äº‹ä»¶è®°å½•åˆ°Stdout\n",
    "> æ³¨æ„:å½“å¯¹è±¡ä¸Šçš„verboseæ ‡å¿—è®¾ç½®ä¸ºtrueæ—¶ï¼Œå³ä½¿æ²¡æœ‰æ˜¾å¼ä¼ é€’ï¼Œä¹Ÿä¼šè°ƒç”¨`StdOutCallbackHandler`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'text': '2\\n1 + 3 = 3\\n1 + 4 = 4\\n1 + 5 = 5\\n1 + 6 = 6\\n1 + 7 = 7\\n1 + 8 = 8\\n1 + 9 = 9\\n1 + 10 = 10\\n\\n2 + 1 = 3\\n2 + 2 = 4\\n2 + 3 = 5\\n2 + 4 = 6\\n2 + 5 = 7\\n2 + 6 = 8\\n2 + 7 = 9\\n2 + 8 = 10\\n2 + 9 = 11\\n2 + 10 = 12\\n\\n3 + 1 = 4\\n3 + 2 = 5\\n3 + 3 = 6\\n3 + 4 = 7\\n3 + 5 = 8\\n3 + 6 = 9\\n3 + 7 = 10\\n3 + 8 = 11\\n3 + 9 = 12\\n3 + 10 = 13\\n\\n4 + 1 = 5\\n4 + 2 = 6\\n4 + 3 = 7\\n4 + 4 = '}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.invoke({\"number\":2})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'text': '3\\n1 + 2 + 3 = 6\\n1 + 2 + 3 + 4 = 10\\n\\n*/\\n\\npublic class Main\\n{\\n\\tpublic static void main(String[] args) {\\n\\t    int n = 4;\\n\\t    int sum = 0;\\n\\t    for(int i = 1; i <= n; i++) {\\n\\t        sum += i;\\n\\t        System.out.println(sum);\\n\\t    }\\n\\t}\\n}\\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.invoke({\"number\":2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2, 'text': '3\\n\\n\\nThis equation is correct. '}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"number\":2}, {\"callbacks\":[handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨å“ªé‡Œä¼ é€’å›è°ƒ\n",
    "åœ¨ä¸¤ä¸ªä¸åŒçš„åœ°æ–¹ï¼ŒAPIä¸­çš„å¤§å¤šæ•°å¯¹è±¡(é“¾ã€æ¨¡å‹ã€å·¥å…·ã€ä»£ç†ç­‰)éƒ½å¯ä»¥ä½¿ç”¨`callbacks`\n",
    "- `Constructor callbacks`: åœ¨æ„é€ å‡½æ•°ä¸­å®šä¹‰ï¼Œä¾‹å¦‚`LLMChain(callbacks=[handler]ï¼Œ tags=['a-tag'])`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›è°ƒå°†ç”¨äºå¯¹è¯¥å¯¹è±¡è¿›è¡Œçš„æ‰€æœ‰è°ƒç”¨ï¼Œå¹¶ä¸”å°†ä»…ä½œç”¨äºè¯¥å¯¹è±¡ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å°†å¤„ç†ç¨‹åºä¼ é€’ç»™LLMChainæ„é€ å‡½æ•°ï¼Œå®ƒå°†ä¸ä¼šè¢«é™„åŠ åˆ°è¯¥é“¾ä¸Šçš„æ¨¡å‹ä½¿ç”¨ã€‚\n",
    "- `Request callbacks`: åœ¨ç”¨äºå‘å‡ºè¯·æ±‚çš„`invoke`æ–¹æ³•ä¸­å®šä¹‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›è°ƒå°†åªç”¨äºç‰¹å®šçš„è¯·æ±‚ï¼Œä»¥åŠå®ƒåŒ…å«çš„æ‰€æœ‰å­è¯·æ±‚(ä¾‹å¦‚ï¼Œå¯¹`LLMChain`çš„è°ƒç”¨è§¦å‘å¯¹Modelçš„è°ƒç”¨ï¼Œè¯¥è°ƒç”¨ä½¿ç”¨åœ¨`invoke()`æ–¹æ³•ä¸­ä¼ é€’çš„ç›¸åŒå¤„ç†ç¨‹åº)ã€‚åœ¨`invoke()`æ–¹æ³•ä¸­ï¼Œå›è°ƒé€šè¿‡é…ç½®å‚æ•°ä¼ é€’ã€‚ä½¿ç”¨`invoke`æ–¹æ³•çš„ç¤ºä¾‹(æ³¨æ„:åŒæ ·çš„æ–¹æ³•å¯ä»¥ç”¨äº`batch`, `ainvoke`å’Œ`abbatch`æ–¹æ³•ã€‚)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "config = {\n",
    "    'callbacks' : [handler]\n",
    "}\n",
    "\n",
    "chain = prompt | chain\n",
    "chain.invoke({\"number\":2}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> æ³¨æ„:`chain =prompt | chain`ç­‰ä»·äº`chain = LLMChain(llm=llm, prompt=prompt)`(æŸ¥çœ‹LangChain Expression Language (LCEL)æ–‡æ¡£äº†è§£æ›´å¤šç»†èŠ‚)\n",
    "\n",
    "`verbose`å‚æ•°åœ¨æ•´ä¸ªAPIä¸­çš„å¤§å¤šæ•°å¯¹è±¡(é“¾ï¼Œæ¨¡å‹ï¼Œå·¥å…·ï¼Œä»£ç†ç­‰)ä¸Šä½œä¸ºæ„é€ å‡½æ•°å‚æ•°å¯ç”¨ï¼Œä¾‹å¦‚`LLMChain(verbose=True)`ï¼Œå®ƒç›¸å½“äºå°†`ConsoleCallbackHandler`ä¼ é€’ç»™è¯¥å¯¹è±¡å’Œæ‰€æœ‰å­å¯¹è±¡çš„å›è°ƒå‚æ•°ã€‚è¿™å¯¹äºè°ƒè¯•å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå°†æŠŠæ‰€æœ‰äº‹ä»¶è®°å½•åˆ°æ§åˆ¶å°ã€‚\n",
    "\n",
    "ä½ æƒ³åœ¨ä»€ä¹ˆæ—¶å€™ä½¿ç”¨å®ƒä»¬\n",
    "- æ„é€ å‡½æ•°å›è°ƒå¯¹äºæ—¥å¿—è®°å½•ã€ç›‘è§†ç­‰ç”¨ä¾‹æœ€æœ‰ç”¨ï¼Œè¿™äº›ç”¨ä¾‹ä¸æ˜¯ç‰¹å®šäºå•ä¸ªè¯·æ±‚ï¼Œè€Œæ˜¯é’ˆå¯¹æ•´ä¸ªé“¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è®°å½•å‘LLMChainå‘å‡ºçš„æ‰€æœ‰è¯·æ±‚ï¼Œæ‚¨å°†å‘æ„é€ å‡½æ•°ä¼ é€’ä¸€ä¸ªå¤„ç†ç¨‹åºã€‚\n",
    "- è¯·æ±‚å›è°ƒå¯¹äºæµè¿™æ ·çš„ç”¨ä¾‹æ˜¯æœ€æœ‰ç”¨çš„ï¼Œå½“ä½ æƒ³è¦å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµå¼ä¼ è¾“åˆ°ç‰¹å®šçš„websocketè¿æ¥æ—¶ï¼Œæˆ–è€…å…¶ä»–ç±»ä¼¼çš„ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å¸Œæœ›å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµå¼ä¼ è¾“åˆ°websocketï¼Œåˆ™éœ€è¦å°†å¤„ç†ç¨‹åºä¼ é€’ç»™invoke()æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async callbacks\n",
    "\n",
    "å¦‚æœä½ æ‰“ç®—ä½¿ç”¨å¼‚æ­¥APIï¼Œå»ºè®®ä½¿ç”¨`AsyncCallbackHandler`æ¥é¿å…é˜»å¡è¿è¡Œå¾ªç¯ã€‚\n",
    "\n",
    "é«˜çº§å¦‚æœæ‚¨åœ¨ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è¿è¡Œ`LLM /chain/tool/agent`æ—¶ä½¿ç”¨`Sync CallbackHandler`ï¼Œåˆ™å®ƒä»ç„¶å¯ä»¥æ­£å¸¸å·¥ä½œã€‚ä½†æ˜¯ï¼Œåœ¨å¼•æ“ç›–ä¸‹ï¼Œå®ƒå°†ä¸`Run_in_executor`ä¸€èµ·è°ƒç”¨ï¼Œå¦‚æœæ‚¨çš„`CallbackHandler`ä¸æ˜¯çº¿ç¨‹å®‰å…¨ï¼Œå¯èƒ½ä¼šå¼•èµ·é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zzzz....\n",
      "Hi! I just woke up. Your llm is starting\n",
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "Sync handler being called in a `thread_pool_executor`: token: Why\n",
      "Sync handler being called in a `thread_pool_executor`: token:  couldn\n",
      "Sync handler being called in a `thread_pool_executor`: token: 't\n",
      "Sync handler being called in a `thread_pool_executor`: token:  the\n",
      "Sync handler being called in a `thread_pool_executor`: token:  bicycle\n",
      "Sync handler being called in a `thread_pool_executor`: token:  stand\n",
      "Sync handler being called in a `thread_pool_executor`: token:  up\n",
      "Sync handler being called in a `thread_pool_executor`: token:  by\n",
      "Sync handler being called in a `thread_pool_executor`: token:  itself\n",
      "Sync handler being called in a `thread_pool_executor`: token: ?\n",
      "\n",
      "\n",
      "Sync handler being called in a `thread_pool_executor`: token: Because\n",
      "Sync handler being called in a `thread_pool_executor`: token:  it\n",
      "Sync handler being called in a `thread_pool_executor`: token:  was\n",
      "Sync handler being called in a `thread_pool_executor`: token:  two\n",
      "Sync handler being called in a `thread_pool_executor`: token:  tired\n",
      "Sync handler being called in a `thread_pool_executor`: token: !\n",
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "zzzz....\n",
      "Hi! I just woke up. Your llm is ending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", response_metadata={'finish_reason': 'stop'}, id='run-368529bc-7212-493d-93b7-8c62a30c6de7-0'))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('368529bc-7212-493d-93b7-8c62a30c6de7'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\"Hi! I just woke up. Your llm is starting\")\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        print(\"Hi! I just woke up. Your llm is ending\")\n",
    "\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in a list with our custom handler\n",
    "chat = ChatOpenAI(\n",
    "    max_tokens=25,\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",
    ")\n",
    "\n",
    "await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callback handlers\n",
    "è¦åˆ›å»ºè‡ªå®šä¹‰å›è°ƒå¤„ç†ç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šæˆ‘ä»¬å¸Œæœ›å›è°ƒå¤„ç†ç¨‹åºå¤„ç†çš„äº‹ä»¶ï¼Œä»¥åŠå½“äº‹ä»¶è¢«è§¦å‘æ—¶æˆ‘ä»¬å¸Œæœ›å›è°ƒå¤„ç†ç¨‹åºåšä»€ä¹ˆã€‚ç„¶åï¼Œæˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯å°†å›è°ƒå¤„ç†ç¨‹åºä½œä¸ºæ„é€ å‡½æ•°å›è°ƒæˆ–è¯·æ±‚å›è°ƒ(å‚è§å›è°ƒç±»å‹)é™„åŠ åˆ°å¯¹è±¡ä¸Šã€‚\n",
    "\n",
    "åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†ç¨‹åºå®ç°æµã€‚\n",
    "\n",
    "åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰å›è°ƒå¤„ç†ç¨‹åº`MyCustomHandler`ä¸­ï¼Œæˆ‘ä»¬å®ç°`on_llm_new_token `æ¥æ‰“å°æˆ‘ä»¬åˆšåˆšæ”¶åˆ°çš„ä»¤ç‰Œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è‡ªå®šä¹‰å¤„ç†ç¨‹åºä½œä¸ºæ„é€ å‡½æ•°å›è°ƒé™„åŠ åˆ°æ¨¡å‹å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My custom handler, token: \n",
      "My custom handler, token: Why\n",
      "My custom handler, token:  did\n",
      "My custom handler, token:  the\n",
      "My custom handler, token:  bear\n",
      "My custom handler, token:  break\n",
      "My custom handler, token:  up\n",
      "My custom handler, token:  with\n",
      "My custom handler, token:  his\n",
      "My custom handler, token:  girlfriend\n",
      "My custom handler, token: ?\n",
      "My custom handler, token:  \n",
      "\n",
      "\n",
      "My custom handler, token: Because\n",
      "My custom handler, token:  he\n",
      "My custom handler, token:  couldn\n",
      "My custom handler, token: 't\n",
      "My custom handler, token:  bear\n",
      "My custom handler, token:  the\n",
      "My custom handler, token:  relationship\n",
      "My custom handler, token:  any\n",
      "My custom handler, token:  longer\n",
      "My custom handler, token: !\n",
      "My custom handler, token: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in our custom handler as a list to the callbacks parameter\n",
    "model = ChatOpenAI(streaming=True, callbacks=[MyCustomHandler()])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"animal\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File logging\n",
    "LangChainæä¾›äº†`FileCallbackHandler`æ¥å°†æ—¥å¿—å†™å…¥æ–‡ä»¶ã€‚`FileCallbackHandler`ç±»ä¼¼äºStdOutCallbackHandlerï¼Œä½†å®ƒä¸æ˜¯å°†æ—¥å¿—æ‰“å°åˆ°æ ‡å‡†è¾“å‡ºï¼Œè€Œæ˜¯å°†æ—¥å¿—å†™å…¥æ–‡ä»¶ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-07 11:28:40.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1m3\n",
      "\n",
      "3 is the sum of 1 and 2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import FileCallbackHandler, StdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler_1 = FileCallbackHandler(logfile)\n",
    "handler_2 = StdOutCallbackHandler()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "model = OpenAI()\n",
    "\n",
    "# this chain will both print to stdout (because verbose=True) and write to 'output.log'\n",
    "# if verbose=False, the FileCallbackHandler will still write to 'output.log'\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"number\": 2}, {\"callbacks\": [handler_1, handler_2]})\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
       "<html>\n",
       "<head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
       "<title></title>\n",
       "<style type=\"text/css\">\n",
       ".ansi2html-content { display: inline; white-space: pre-wrap; word-wrap: break-word; }\n",
       ".body_foreground { color: #AAAAAA; }\n",
       ".body_background { background-color: #000000; }\n",
       ".inv_foreground { color: #000000; }\n",
       ".inv_background { background-color: #AAAAAA; }\n",
       "</style>\n",
       "</head>\n",
       "<body class=\"body_foreground body_background\" style=\"font-size: normal;\" >\n",
       "<pre class=\"ansi2html-content\">\n",
       "meow meowğŸ± \n",
       " meow meowğŸ± \n",
       " meowğŸ˜»ğŸ˜»\n",
       "</pre>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ansi2html import Ansi2HTMLConverter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "with open(\"data/meow.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "conv = Ansi2HTMLConverter()\n",
    "html = conv.convert(content, full=True)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple callback handlers\n",
    "åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨callbacks=åœ¨å¯¹è±¡åˆ›å»ºæ—¶ä¼ å…¥å›è°ƒå¤„ç†ç¨‹åºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›è°ƒå°†é™å®šåœ¨è¯¥ç‰¹å®šå¯¹è±¡çš„èŒƒå›´å†…ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œåœ¨è¿è¡Œå¯¹è±¡æ—¶ä¼ é€’å¤„ç†ç¨‹åºæ˜¯æœ‰åˆ©çš„ã€‚å½“æˆ‘ä»¬åœ¨æ‰§è¡Œè¿è¡Œæ—¶ä½¿ç”¨`callback`å…³é”®å­—`arg`ä¼ é€’`CallbackHandlers`æ—¶ï¼Œè¿™äº›å›è°ƒå°†ç”±æ‰§è¡Œä¸­æ¶‰åŠçš„æ‰€æœ‰åµŒå¥—å¯¹è±¡å‘å‡ºã€‚ä¾‹å¦‚ï¼Œå½“å¤„ç†ç¨‹åºä¼ é€’ç»™ä»£ç†æ—¶ï¼Œå®ƒå°†ç”¨äºä¸ä»£ç†ç›¸å…³çš„æ‰€æœ‰å›è°ƒä»¥åŠä»£ç†æ‰§è¡Œä¸­æ¶‰åŠçš„æ‰€æœ‰å¯¹è±¡(åœ¨æœ¬ä¾‹ä¸­ä¸º`Tools`ã€`LLMChain`å’Œ`LLM`)ã€‚\n",
    "\n",
    "è¿™ä½¿æˆ‘ä»¬ä¸å¿…æ‰‹åŠ¨å°†å¤„ç†ç¨‹åºé™„åŠ åˆ°æ¯ä¸ªå•ç‹¬çš„åµŒå¥—å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start AgentExecutor\n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token  I\n",
      "on_new_token  should\n",
      "on_new_token  use\n",
      "on_new_token  a\n",
      "on_new_token  calculator\n",
      "on_new_token  to\n",
      "on_new_token  solve\n",
      "on_new_token  this\n",
      "on_new_token .\n",
      "Action\n",
      "on_new_token :\n",
      "on_new_token  Calculator\n",
      "on_new_token \n",
      "Action\n",
      "on_new_token  Input\n",
      "on_new_token :\n",
      "on_new_token  \n",
      "on_new_token 2\n",
      "on_new_token ^\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "on_agent_action tool='Calculator' tool_input='2^0.235' log=' I should use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2^0.235'\n",
      "on_tool_start Calculator\n",
      "on_chain_start LLMMathChain\n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token ```text\n",
      "on_new_token \n",
      "\n",
      "on_new_token 2\n",
      "on_new_token **\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "\n",
      "on_new_token ```\n",
      "\n",
      "on_new_token ...\n",
      "on_new_token num\n",
      "on_new_token expr\n",
      "on_new_token .evaluate\n",
      "on_new_token (\"\n",
      "on_new_token 2\n",
      "on_new_token **\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \")\n",
      "on_new_token ...\n",
      "\n",
      "on_new_token \n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token  I\n",
      "on_new_token  now\n",
      "on_new_token  know\n",
      "on_new_token  the\n",
      "on_new_token  final\n",
      "on_new_token  answer\n",
      "on_new_token .\n",
      "Final Answer:\n",
      "on_new_token  \n",
      "on_new_token 1.\n",
      "on_new_token 1769067372187674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1769067372187674'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "# First, define custom callback handler implementations\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "\n",
    "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n",
    "\n",
    "\n",
    "# Instantiate the handlers\n",
    "handler1 = MyCustomHandlerOne()\n",
    "handler2 = MyCustomHandlerTwo()\n",
    "\n",
    "# Setup the agent. Only the `llm` will issue callbacks for handler2\n",
    "llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "# Callbacks for handler1 will be issued by every object involved in the\n",
    "# Agent execution (llm, llmchain, tool, agent executor)\n",
    "agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "æ‚¨å¯ä»¥é€šè¿‡å°†æ ‡ç­¾å‚æ•°ä¼ é€’ç»™`call()/run()/apply()`æ–¹æ³•æ¥å°†æ ‡ç­¾æ·»åŠ åˆ°å›è°ƒä¸­ã€‚è¿™å¯¹äºè¿‡æ»¤æ‚¨çš„æ—¥å¿—å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚å¦‚æœè¦å°†æ‰€æœ‰è¯·æ±‚è®°å½•åˆ°ç‰¹å®šçš„llmchainï¼Œåˆ™å¯ä»¥æ·»åŠ æ ‡ç­¾ï¼Œç„¶åé€šè¿‡è¯¥æ ‡ç­¾è¿‡æ»¤æ‚¨çš„æ—¥å¿—ã€‚æ‚¨å¯ä»¥å°†æ ‡ç­¾ä¼ é€’ç»™æ„é€ å‡½æ•°å’Œè¯·æ±‚å›è°ƒï¼Œæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä¸Šé¢çš„ç¤ºä¾‹ã€‚ç„¶åï¼Œè¿™äº›æ ‡ç­¾å°†ä¼ é€’ç»™â€œå¯åŠ¨â€å›è°ƒæ–¹æ³•çš„æ ‡ç­¾å‚æ•°ï¼Œå³ã€‚`on_llm_start`ï¼Œ`on_chat_model_start`ï¼Œ`on_chain_start`ï¼Œ`on_tool_start`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token counting\n",
    "LangChainæä¾›äº†ä¸€ä¸ªå…è®¸è®¡æ•°ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "with get_openai_callback() as cb:\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can kick off concurrent runs from within the context manager\n",
    "with get_openai_callback() as cb:\n",
    "    await asyncio.gather(\n",
    "        *[llm.agenerate([\"What is the square root of 4?\"]) for _ in range(3)]\n",
    "    )\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The context manager is concurrency safe\n",
    "task = asyncio.create_task(llm.agenerate([\"What is the square root of 4?\"]))\n",
    "with get_openai_callback() as cb:\n",
    "    await llm.agenerate([\"What is the square root of 4?\"])\n",
    "\n",
    "await task\n",
    "cb.total_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
