{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "LangChain提供了一个回调系统，允许您挂钩到LLM应用程序的各个阶段。这对于日志记录、监视、流处理和其他任务非常有用\n",
    "\n",
    "您可以通过使用整个API中可用的callback参数来订阅这些事件。该参数是处理程序对象的列表，这些对象将实现下面更详细描述的一个或多个方法。\n",
    "\n",
    "## Callback handlers\n",
    "`CallbackHandlers`是实现`CallbackHandler`接口的对象，该接口为每个可以订阅的事件提供一个方法。当事件被触发时，`CallbackManager`将在每个处理程序上调用适当的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.agents import AgentFinish, AgentAction\n",
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "Langchain提供了一些内置处理程序，您可以使用这些处理程序来开始。这些可在langchain_core/回调模块中可用。最基本的处理程序是`StdOutCallbackHandler`，它只是将所有事件记录到Stdout\n",
    "> 注意:当对象上的verbose标志设置为true时，即使没有显式传递，也会调用`StdOutCallbackHandler`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'text': '2\\n1 + 3 = 3\\n1 + 4 = 4\\n1 + 5 = 5\\n1 + 6 = 6\\n1 + 7 = 7\\n1 + 8 = 8\\n1 + 9 = 9\\n1 + 10 = 10\\n\\n2 + 1 = 3\\n2 + 2 = 4\\n2 + 3 = 5\\n2 + 4 = 6\\n2 + 5 = 7\\n2 + 6 = 8\\n2 + 7 = 9\\n2 + 8 = 10\\n2 + 9 = 11\\n2 + 10 = 12\\n\\n3 + 1 = 4\\n3 + 2 = 5\\n3 + 3 = 6\\n3 + 4 = 7\\n3 + 5 = 8\\n3 + 6 = 9\\n3 + 7 = 10\\n3 + 8 = 11\\n3 + 9 = 12\\n3 + 10 = 13\\n\\n4 + 1 = 5\\n4 + 2 = 6\\n4 + 3 = 7\\n4 + 4 = '}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.invoke({\"number\":2})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'text': '3\\n1 + 2 + 3 = 6\\n1 + 2 + 3 + 4 = 10\\n\\n*/\\n\\npublic class Main\\n{\\n\\tpublic static void main(String[] args) {\\n\\t    int n = 4;\\n\\t    int sum = 0;\\n\\t    for(int i = 1; i <= n; i++) {\\n\\t        sum += i;\\n\\t        System.out.println(sum);\\n\\t    }\\n\\t}\\n}\\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.invoke({\"number\":2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'number': 2, 'text': '3\\n\\n\\nThis equation is correct. '}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"number\":2}, {\"callbacks\":[handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在哪里传递回调\n",
    "在两个不同的地方，API中的大多数对象(链、模型、工具、代理等)都可以使用`callbacks`\n",
    "- `Constructor callbacks`: 在构造函数中定义，例如`LLMChain(callbacks=[handler]， tags=['a-tag'])`。在这种情况下，回调将用于对该对象进行的所有调用，并且将仅作用于该对象，例如，如果您将处理程序传递给LLMChain构造函数，它将不会被附加到该链上的模型使用。\n",
    "- `Request callbacks`: 在用于发出请求的`invoke`方法中定义。在这种情况下，回调将只用于特定的请求，以及它包含的所有子请求(例如，对`LLMChain`的调用触发对Model的调用，该调用使用在`invoke()`方法中传递的相同处理程序)。在`invoke()`方法中，回调通过配置参数传递。使用`invoke`方法的示例(注意:同样的方法可以用于`batch`, `ainvoke`和`abbatch`方法。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "config = {\n",
    "    'callbacks' : [handler]\n",
    "}\n",
    "\n",
    "chain = prompt | chain\n",
    "chain.invoke({\"number\":2}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意:`chain =prompt | chain`等价于`chain = LLMChain(llm=llm, prompt=prompt)`(查看LangChain Expression Language (LCEL)文档了解更多细节)\n",
    "\n",
    "`verbose`参数在整个API中的大多数对象(链，模型，工具，代理等)上作为构造函数参数可用，例如`LLMChain(verbose=True)`，它相当于将`ConsoleCallbackHandler`传递给该对象和所有子对象的回调参数。这对于调试很有用，因为它将把所有事件记录到控制台。\n",
    "\n",
    "你想在什么时候使用它们\n",
    "- 构造函数回调对于日志记录、监视等用例最有用，这些用例不是特定于单个请求，而是针对整个链。例如，如果您想记录向LLMChain发出的所有请求，您将向构造函数传递一个处理程序。\n",
    "- 请求回调对于流这样的用例是最有用的，当你想要将单个请求的输出流式传输到特定的websocket连接时，或者其他类似的用例。例如，如果您希望将单个请求的输出流式传输到websocket，则需要将处理程序传递给invoke()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async callbacks\n",
    "\n",
    "如果你打算使用异步API，建议使用`AsyncCallbackHandler`来避免阻塞运行循环。\n",
    "\n",
    "高级如果您在使用异步方法运行`LLM /chain/tool/agent`时使用`Sync CallbackHandler`，则它仍然可以正常工作。但是，在引擎盖下，它将与`Run_in_executor`一起调用，如果您的`CallbackHandler`不是线程安全，可能会引起问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zzzz....\n",
      "Hi! I just woke up. Your llm is starting\n",
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "Sync handler being called in a `thread_pool_executor`: token: Why\n",
      "Sync handler being called in a `thread_pool_executor`: token:  couldn\n",
      "Sync handler being called in a `thread_pool_executor`: token: 't\n",
      "Sync handler being called in a `thread_pool_executor`: token:  the\n",
      "Sync handler being called in a `thread_pool_executor`: token:  bicycle\n",
      "Sync handler being called in a `thread_pool_executor`: token:  stand\n",
      "Sync handler being called in a `thread_pool_executor`: token:  up\n",
      "Sync handler being called in a `thread_pool_executor`: token:  by\n",
      "Sync handler being called in a `thread_pool_executor`: token:  itself\n",
      "Sync handler being called in a `thread_pool_executor`: token: ?\n",
      "\n",
      "\n",
      "Sync handler being called in a `thread_pool_executor`: token: Because\n",
      "Sync handler being called in a `thread_pool_executor`: token:  it\n",
      "Sync handler being called in a `thread_pool_executor`: token:  was\n",
      "Sync handler being called in a `thread_pool_executor`: token:  two\n",
      "Sync handler being called in a `thread_pool_executor`: token:  tired\n",
      "Sync handler being called in a `thread_pool_executor`: token: !\n",
      "Sync handler being called in a `thread_pool_executor`: token: \n",
      "zzzz....\n",
      "Hi! I just woke up. Your llm is ending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", response_metadata={'finish_reason': 'stop'}, id='run-368529bc-7212-493d-93b7-8c62a30c6de7-0'))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('368529bc-7212-493d-93b7-8c62a30c6de7'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\"Hi! I just woke up. Your llm is starting\")\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        print(\"Hi! I just woke up. Your llm is ending\")\n",
    "\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in a list with our custom handler\n",
    "chat = ChatOpenAI(\n",
    "    max_tokens=25,\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",
    ")\n",
    "\n",
    "await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callback handlers\n",
    "要创建自定义回调处理程序，我们需要确定我们希望回调处理程序处理的事件，以及当事件被触发时我们希望回调处理程序做什么。然后，我们所需要做的就是将回调处理程序作为构造函数回调或请求回调(参见回调类型)附加到对象上。\n",
    "\n",
    "在下面的示例中，我们将使用自定义处理程序实现流。\n",
    "\n",
    "在我们的自定义回调处理程序`MyCustomHandler`中，我们实现`on_llm_new_token `来打印我们刚刚收到的令牌。然后，我们将自定义处理程序作为构造函数回调附加到模型对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My custom handler, token: \n",
      "My custom handler, token: Why\n",
      "My custom handler, token:  did\n",
      "My custom handler, token:  the\n",
      "My custom handler, token:  bear\n",
      "My custom handler, token:  break\n",
      "My custom handler, token:  up\n",
      "My custom handler, token:  with\n",
      "My custom handler, token:  his\n",
      "My custom handler, token:  girlfriend\n",
      "My custom handler, token: ?\n",
      "My custom handler, token:  \n",
      "\n",
      "\n",
      "My custom handler, token: Because\n",
      "My custom handler, token:  he\n",
      "My custom handler, token:  couldn\n",
      "My custom handler, token: 't\n",
      "My custom handler, token:  bear\n",
      "My custom handler, token:  the\n",
      "My custom handler, token:  relationship\n",
      "My custom handler, token:  any\n",
      "My custom handler, token:  longer\n",
      "My custom handler, token: !\n",
      "My custom handler, token: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in our custom handler as a list to the callbacks parameter\n",
    "model = ChatOpenAI(streaming=True, callbacks=[MyCustomHandler()])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"animal\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File logging\n",
    "LangChain提供了`FileCallbackHandler`来将日志写入文件。`FileCallbackHandler`类似于StdOutCallbackHandler，但它不是将日志打印到标准输出，而是将日志写入文件。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-07 11:28:40.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1m3\n",
      "\n",
      "3 is the sum of 1 and 2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import FileCallbackHandler, StdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler_1 = FileCallbackHandler(logfile)\n",
    "handler_2 = StdOutCallbackHandler()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "model = OpenAI()\n",
    "\n",
    "# this chain will both print to stdout (because verbose=True) and write to 'output.log'\n",
    "# if verbose=False, the FileCallbackHandler will still write to 'output.log'\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"number\": 2}, {\"callbacks\": [handler_1, handler_2]})\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
       "<html>\n",
       "<head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
       "<title></title>\n",
       "<style type=\"text/css\">\n",
       ".ansi2html-content { display: inline; white-space: pre-wrap; word-wrap: break-word; }\n",
       ".body_foreground { color: #AAAAAA; }\n",
       ".body_background { background-color: #000000; }\n",
       ".inv_foreground { color: #000000; }\n",
       ".inv_background { background-color: #AAAAAA; }\n",
       "</style>\n",
       "</head>\n",
       "<body class=\"body_foreground body_background\" style=\"font-size: normal;\" >\n",
       "<pre class=\"ansi2html-content\">\n",
       "meow meow🐱 \n",
       " meow meow🐱 \n",
       " meow😻😻\n",
       "</pre>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ansi2html import Ansi2HTMLConverter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "with open(\"data/meow.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "conv = Ansi2HTMLConverter()\n",
    "html = conv.convert(content, full=True)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple callback handlers\n",
    "在前面的示例中，我们通过使用callbacks=在对象创建时传入回调处理程序。在这种情况下，回调将限定在该特定对象的范围内。\n",
    "\n",
    "然而，在许多情况下，在运行对象时传递处理程序是有利的。当我们在执行运行时使用`callback`关键字`arg`传递`CallbackHandlers`时，这些回调将由执行中涉及的所有嵌套对象发出。例如，当处理程序传递给代理时，它将用于与代理相关的所有回调以及代理执行中涉及的所有对象(在本例中为`Tools`、`LLMChain`和`LLM`)。\n",
    "\n",
    "这使我们不必手动将处理程序附加到每个单独的嵌套对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "d:\\Users\\DELL\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start AgentExecutor\n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token  I\n",
      "on_new_token  should\n",
      "on_new_token  use\n",
      "on_new_token  a\n",
      "on_new_token  calculator\n",
      "on_new_token  to\n",
      "on_new_token  solve\n",
      "on_new_token  this\n",
      "on_new_token .\n",
      "Action\n",
      "on_new_token :\n",
      "on_new_token  Calculator\n",
      "on_new_token \n",
      "Action\n",
      "on_new_token  Input\n",
      "on_new_token :\n",
      "on_new_token  \n",
      "on_new_token 2\n",
      "on_new_token ^\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "on_agent_action tool='Calculator' tool_input='2^0.235' log=' I should use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2^0.235'\n",
      "on_tool_start Calculator\n",
      "on_chain_start LLMMathChain\n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token ```text\n",
      "on_new_token \n",
      "\n",
      "on_new_token 2\n",
      "on_new_token **\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "\n",
      "on_new_token ```\n",
      "\n",
      "on_new_token ...\n",
      "on_new_token num\n",
      "on_new_token expr\n",
      "on_new_token .evaluate\n",
      "on_new_token (\"\n",
      "on_new_token 2\n",
      "on_new_token **\n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \")\n",
      "on_new_token ...\n",
      "\n",
      "on_new_token \n",
      "on_chain_start LLMChain\n",
      "on_llm_start OpenAI\n",
      "on_llm_start (I'm the second handler!!) OpenAI\n",
      "on_new_token  I\n",
      "on_new_token  now\n",
      "on_new_token  know\n",
      "on_new_token  the\n",
      "on_new_token  final\n",
      "on_new_token  answer\n",
      "on_new_token .\n",
      "Final Answer:\n",
      "on_new_token  \n",
      "on_new_token 1.\n",
      "on_new_token 1769067372187674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1769067372187674'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "# First, define custom callback handler implementations\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "\n",
    "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n",
    "\n",
    "\n",
    "# Instantiate the handlers\n",
    "handler1 = MyCustomHandlerOne()\n",
    "handler2 = MyCustomHandlerTwo()\n",
    "\n",
    "# Setup the agent. Only the `llm` will issue callbacks for handler2\n",
    "llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "# Callbacks for handler1 will be issued by every object involved in the\n",
    "# Agent execution (llm, llmchain, tool, agent executor)\n",
    "agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "您可以通过将标签参数传递给`call()/run()/apply()`方法来将标签添加到回调中。这对于过滤您的日志很有用，例如如果要将所有请求记录到特定的llmchain，则可以添加标签，然后通过该标签过滤您的日志。您可以将标签传递给构造函数和请求回调，有关详细信息，请参见上面的示例。然后，这些标签将传递给“启动”回调方法的标签参数，即。`on_llm_start`，`on_chat_model_start`，`on_chain_start`，`on_tool_start`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token counting\n",
    "LangChain提供了一个允许计数令牌的上下文管理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "with get_openai_callback() as cb:\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "    llm.invoke(\"What is the square root of 4?\")\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can kick off concurrent runs from within the context manager\n",
    "with get_openai_callback() as cb:\n",
    "    await asyncio.gather(\n",
    "        *[llm.agenerate([\"What is the square root of 4?\"]) for _ in range(3)]\n",
    "    )\n",
    "\n",
    "cb.total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The context manager is concurrency safe\n",
    "task = asyncio.create_task(llm.agenerate([\"What is the square root of 4?\"]))\n",
    "with get_openai_callback() as cb:\n",
    "    await llm.agenerate([\"What is the square root of 4?\"])\n",
    "\n",
    "await task\n",
    "cb.total_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
