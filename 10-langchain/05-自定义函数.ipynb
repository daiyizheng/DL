{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义函数（ [RunnableConfig](https://python.langchain.com/docs/expression_language/primitives/functions/)）\n",
    "\n",
    "您可以在管道中使用任意函数。请注意，这些函数的所有输入都需要是单个参数。如果您有一个接受多个参数的函数，您应该编写一个包装器来接受单个输入并将其解包为多个参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `RunnableLambda`允许你将任意函数包装为一个`Runnable`，可以参与`pipeline`运算。传入`RunnableLambda`的函数必须只接受一个参数，如果本身是多参数的，要写一个`wrapper`，接受一个参数后解包传递。\n",
    "\n",
    "  下面我们义`length_function`函数计算一个字符串的长度，`multiple_length_function`函数计算两个字符串长度的乘积。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-43035dab-bdda-4760-85a2-4fa54736b1b6-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码中，`chain`的第一步是一个字典，通过两层运算，将键a和b的值填充到prompt模板。比如对于键a：\n",
    "\n",
    "- 使用`itemgetter`提取出需要计算的字符串\"bar\"\n",
    "- 用`RunnableLambda`包装的函数计算字符串长度为3。\n",
    "- 结果填充到`prompt`中，即a=3\n",
    "\n",
    "  所以这段代码展示了如何在参数填充阶段，利用自定义函数进行复杂的数据预处理和计算，最终 Feed 到 prompting 的过程。整个链的关键就是插入了 `RunnableLambda enables` 我们插入自定义运算逻辑。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accepting a Runnable Config\n",
    "`Runnable lambdas` 可以选择性地接受`RunnableConfig`，它们可以使用`RunnableConfig`将回调、标记和其他配置信息传递给嵌套运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI()\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n",
      "Tokens Used: 65\n",
      "\tPrompt Tokens: 56\n",
      "\tCompletion Tokens: 9\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00010200000000000001\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "    print(output)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流式自定义生成器函数\n",
    "流式自定义生成器函数允许我们在`LCEL pipeline`中使用`yield`生成器函数作为自定义的输出解析器或中间处理步骤，同时保持流计算的特性。生成器函数的签名应该是`Iterator[Input] -> Iterator[Output]`，异步生成器应该是`AsyncIterator[Input] -> AsyncIterator[Output]`。\n",
    "\n",
    "  流式自定义生成器函数主要有两个应用：\n",
    "\n",
    "- 自定义输出解析器\n",
    "- 不打破流计算的前提下处理流中的数据。\n",
    " \n",
    "下面举一个示例，定义一个自定义的输出解析器`split_into_list`，来把`ChatGPT`的标记流解析为字符串列表。|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Wolf\n",
      "2. Tiger\n",
      "3. Lion\n",
      "4. Gorilla\n",
      "5. Panda"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a comma-separated list of 5 animals similar to: {animal}\"\n",
    ")\n",
    "model = ChatOpenAI(temperature=0.0)\n",
    "str_chain = prompt | model | StrOutputParser()\n",
    "\n",
    "for chunk in str_chain.stream({\"animal\": \"bear\"}):\n",
    "    print(chunk, end=\"\", flush=True)  \t\t\t\t\t# 输出：lion, tiger, wolf, gorilla, panda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者是使用invoke方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_chain.invoke({\"animal\": \"bear\"})\t\t\t\t\t# 输出：lion, tiger, wolf, gorilla, panda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义一个自定义的解析器,将llm标记流分割成以逗号分隔的字符串列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入的迭代器拆分成以逗号分隔的字符串列表\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    \n",
    "    buffer = \"\"\t\t\t\t\t\t\t\t\t\t# 保存部分输入直到遇到逗号\n",
    "    \n",
    "    for chunk in input:            \t\t\t\t\t# 遍历输入的标记流迭代器input,每次将当前chunk添加到buffer\n",
    "        buffer += chunk\t\t\t\t\t\t\t\t             \n",
    "        while \",\" in buffer:      \t\t\t\t\t# 如果缓冲区中有逗号，获取逗号的索引              \n",
    "            comma_index = buffer.index(\",\")    \t\t                    \n",
    "            yield [buffer[:comma_index].strip()]\t# 生成逗号前的所有内容            \n",
    "            buffer = buffer[comma_index + 1 :]\t\t# 将逗号后面的内容保存给下一次迭代    \n",
    "    yield [buffer.strip()]\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要逻辑：\n",
    "\n",
    "- 定义一个`buffer`字符串用于保存每次迭代读取的`chunk`\n",
    "- 遍历输入的标记流迭代器`input`，每次将`chunk`添加到`buffer`\n",
    "- 如果`buffer`中包含逗号，则\n",
    "- 获取逗号的索引\n",
    "- 将逗号前的内容取出，去空格后放进列表`yield`\n",
    "- 将逗号后的内容留在`buffer`，等待下次迭代\n",
    "- 最后一个`chunk`也做同样的解析`yield`出去\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Wolf\\n2. Lion\\n3. Tiger\\n4. Gorilla\\n5. Panda']\n"
     ]
    }
   ],
   "source": [
    "list_chain = str_chain | split_into_list\n",
    "for chunk in list_chain.stream({\"animal\": \"bear\"}):\n",
    "    print(chunk, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
