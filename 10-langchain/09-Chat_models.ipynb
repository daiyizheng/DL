{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "\n",
    "聊天模型是语言模型的一种变体。虽然聊天模型在底层使用语言模型，但它们使用的界面略有不同。他们没有使用文本输入和文本输出API，而是使用聊天消息作为输入和输出的接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "pip install -qU langchain-openai\n",
    "pip install -qU langchain-anthropic\n",
    "pip install -qU langchain-google-vertexai\n",
    "pip install -qU langchain-cohere\n",
    "pip install -qU langchain-fireworks\n",
    "pip install -qU langchain-mistralai\n",
    "pip install -qU langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聊天模型界面基于消息而不是原始文本。LangChain目前支持的消息类型有`AIMessage`、`HumanMessage`、`SystemMessage`、`FunctionMessage`和`ChatMessage`。`ChatMessage`接受任意角色参数。大多数时候，你只需要处理`HumanMessage`, `AIMessage`和`SystemMessage`\n",
    "\n",
    "## LCEL\n",
    "聊天模型实现了` Runnable interface`，这是语言链表达语言(LCEL)的基本构建块。这意味着它们支持`invoke`, `ainvoke`, `stream`, `aststream`, `batch`,` abbatch`, `aststream`日志调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "chat.invoke(messages)\n",
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "chat.batch([messages])\n",
    "await chat.ainvoke(messages)\n",
    "async for chunk in chat.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "async for chunk in chat.astream_log(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate\n",
    "批处理调用，更丰富的输出您可以更进一步，使用`generate`为多组消息生成补全。这将返回带有附加`message`的`LLMResult`。除了返回的消息之外，这将包括关于每代的附加信息(例如完成原因)和关于完整API调用的附加信息(例如使用的令牌总数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that translates English to French.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"I love programming.\"),\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that translates English to French.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\"),\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消息类型\n",
    "ChatModels将消息列表作为输入并返回一条消息。有几种不同类型的消息。所有消息都有一个`role`和一个`content`属性。`role`描述了谁的消息。对于不同的角色，LangChain有不同的消息类。`content`属性描述消息的内容。这可以是一些不同的事情\n",
    "- 字符串(大多数模型处理这种类型的内容)\n",
    "- 字典列表(用于多模式输入，其中字典包含有关输入类型和输入位置的信息)\n",
    "\n",
    "此外，消息还有一个`additional_kwargs`属性。在这里可以传递关于消息的附加信息。这主要用于特定于提供程序而非通用的输入参数。最著名的例子是OpenAI的`function_call `。\n",
    "- HumanMessage 这表示来自用户的消息。一般只由内容组成\n",
    "- AIMessage  这表示来自模型的消息。这可能会有`additional_kwargs ` -例如，如果使用OpenAI工具调用`tool_calls `。\n",
    "- SystemMessage 这表示一个系统消息，它告诉模型如何行为。这通常只包括内容。并不是每个模型都支持这一点。\n",
    "- FunctionMessage 这表示函数调用的结果。除了`role`和`content`之外，该消息还有一个name参数，该参数表示为产生此结果而调用的函数的名称。\n",
    "- ToolMessage 这表示工具调用的结果。为了匹配OpenAI的`function`和`tool`消息类型，这与`FunctionMessage`不同。除了`role`和`content`之外，该消息还有一个`tool_call_id `参数，该参数传递了为产生此结果而调用的工具的调用id。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "所有`chatmodel`都实现了`Runnable`接口，该接口与所有方法的默认实现一起提供。`Ainvoke`,`batch`, `abbatch`, `stream`, `astream`。这为所有`chatmodel`提供了对流的基本支持。\n",
    "\n",
    "查看哪些[ integrations ](https://python.langchain.com/docs/integrations/chat/)支持逐个令牌的stream。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAnthropic\n",
    "chat = ChatAnthropic(model=\"claude-2\")\n",
    "for chunk in chat.stream(\"Write me a song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "> 我们交替使用术语工具调用和函数调用。虽然函数调用有时指的是对单个函数的调用，但我们将所有模型视为它们可以在每个消息中返回多个工具或函数调用。\n",
    "\n",
    "工具调用包括名称、参数字典和可选标识符。参数字典是结构化的`{argument_name: argument_value}.`。\n",
    "\n",
    "许多LLM提供商，包括人类，Cohere，Google，Mistral，OpenAI等，都支持工具调用功能的变体。这些功能通常允许向LLM的请求包含可用工具及其模式，并供应包括对这些工具的呼叫。例如，给定搜索引擎工具，LLM可能首先发出搜索引擎的调用来处理查询。呼叫LLM的系统可以接收工具调用，执行该工具并将输出返回LLM以告知其响应。Langchain包括一套内置工具，并支持定义您自己的自定义工具的几种方法。工具呼叫对于建造工具的链条和代理非常有用，并且对于从模型中获取结构化输出非常有用。\n",
    "\n",
    "提供程序采用不同的约定来格式化工具模式和工具调用。例如，Anthropic将工具调用作为较大内容块中的解析结构返回\n",
    "\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"<thinking>\\nI should use a tool.\\n</thinking>\",\n",
    "    \"type\": \"text\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"id_value\",\n",
    "    \"input\": {\"arg_name\": \"arg_value\"},\n",
    "    \"name\": \"tool_name\",\n",
    "    \"type\": \"tool_use\"\n",
    "  }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "而OpenAI将工具调用分离为一个单独的参数，参数为JSON字符串\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"tool_calls\": [\n",
    "    {\n",
    "      \"id\": \"id_value\",\n",
    "      \"function\": {\n",
    "        \"arguments\": '{\"arg_name\": \"arg_value\"}',\n",
    "        \"name\": \"tool_name\"\n",
    "      },\n",
    "      \"type\": \"function\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "LangChain实现了定义工具、将工具传递给llm以及表示工具调用的标准接口。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing tools to LLMs\n",
    "\n",
    "支持工具调用特性的聊天模型实现了一个`.bind_tools`方法，该方法接收LangChain工具对象列表，并以期望的格式将它们绑定到聊天模型。聊天模型的后续调用将在其对LLM的调用中包含工具模式。\n",
    "\n",
    "例如，我们可以在Python函数上使用@tool装饰器来定义自定义工具的模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们使用Pydantic定义模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class Add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [Add, Multiply]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
