{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location-Aware (Location Sensitive) Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationAwareAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a location-aware attention mechanism on the output features from the decoder.\n",
    "    Location-aware attention proposed in \"Attention-Based Models for Speech Recognition\" paper.\n",
    "    The location-aware attention mechanism is performing well in speech recognition tasks.\n",
    "    We refer to implementation of ClovaCall Attention style.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim (int): dimesion of hidden state vector\n",
    "        smoothing (bool): flag indication whether to use smoothing or not.\n",
    "\n",
    "    Inputs: query, value, last_attn, smoothing\n",
    "        - **query** (batch, q_len, hidden_dim): tensor containing the output features from the decoder.\n",
    "        - **value** (batch, v_len, hidden_dim): tensor containing features of the encoded input sequence.\n",
    "        - **last_attn** (batch_size * num_heads, v_len): tensor containing previous timestep`s attention (alignment)\n",
    "\n",
    "    Returns: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the feature from encoder outputs\n",
    "        - **attn** (batch * num_heads, v_len): tensor containing the attention (alignment) from the encoder outputs.\n",
    "\n",
    "    Reference:\n",
    "        - **Attention-Based Models for Speech Recognition**: https://arxiv.org/abs/1506.07503\n",
    "        - **ClovaCall**: https://github.com/clovaai/ClovaCall/blob/master/las.pytorch/models/attention.py\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, smoothing: bool = True) -> None:\n",
    "        super(LocationAwareAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.score_proj = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        self.bias = nn.Parameter(torch.rand(hidden_dim).uniform_(-0.1, 0.1))\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, query: Tensor, value: Tensor, last_attn: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        batch_size, hidden_dim, seq_len = query.size(0), query.size(2), value.size(1)\n",
    "\n",
    "        # Initialize previous attention (alignment) to zeros\n",
    "        if last_attn is None:\n",
    "            last_attn = value.new_zeros(batch_size, seq_len)\n",
    "\n",
    "        conv_attn = torch.transpose(self.conv1d(last_attn.unsqueeze(1)), 1, 2)\n",
    "        score = self.score_proj(torch.tanh(\n",
    "                self.query_proj(query.reshape(-1, hidden_dim)).view(batch_size, -1, hidden_dim)\n",
    "                + self.value_proj(value.reshape(-1, hidden_dim)).view(batch_size, -1, hidden_dim)\n",
    "                + conv_attn\n",
    "                + self.bias\n",
    "        )).squeeze(dim=-1)\n",
    "\n",
    "        if self.smoothing:\n",
    "            score = torch.sigmoid(score)\n",
    "            attn = torch.div(score, score.sum(dim=-1).unsqueeze(dim=-1))\n",
    "        else:\n",
    "            attn = F.softmax(score, dim=-1)\n",
    "\n",
    "        context = torch.bmm(attn.unsqueeze(dim=1), value).squeeze(dim=1)  # Bx1xT X BxTxD => Bx1xD => BxD\n",
    "\n",
    "        return context, attn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
