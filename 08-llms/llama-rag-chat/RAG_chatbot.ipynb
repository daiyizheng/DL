{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2435c44b-e6d0-4c41-adcc-df19ec011abc",
   "metadata": {},
   "source": [
    "本案例介绍了如何构建一个基于Llama 2的聊天机器人，该机器人能够在浏览器上运行，并能够根据自己的数据回答问题。主要内容包括：\n",
    "\n",
    "1. **部署Llama 2 7B**：使用文本生成推理框架将Llama 2 7B作为API服务器部署。\n",
    "2. **构建聊天机器人**：使用Gradio构建聊天机器人，并连接到服务器。\n",
    "3. **增加检索增强生成（RAG）功能**：基于入门指南，增加Llama 2特定知识的RAG能力。\n",
    "4. **RAG架构**：RAG是Meta在2020年发明的一种流行方法，用于增强大型语言模型（LLM）。它通过从外部模型检索数据，并将检索到的相关数据作为上下文添加到LLM的问题或提示中，来回答关于自己数据的问题，或者在LLM训练时不公开的数据问题。\n",
    "\n",
    "RAG的优点是能够保持企业的敏感数据在本地，并且在不需要对模型进行特定角色的微调的情况下，从通用模型中获得更相关的答案，大大减少了模型在回答生成中的幻觉现象。\n",
    "\n",
    "**开发RAG支持的Llama 2聊天机器人的方法**：\n",
    "最简单的方法是使用如LangChain和LlamaIndex这样的框架。这两个开源框架都提供了实现Llama 2的RAG功能的方便API，包括：\n",
    "\n",
    "- 加载和分割文档\n",
    "- 嵌入和存储文档分割\n",
    "- 根据用户查询检索相关上下文\n",
    "- 调用Llama 2进行查询和上下文生成答案\n",
    "\n",
    "LangChain是一个更通用、更灵活的用于开发带有RAG能力的LLM应用程序的框架，而LlamaIndex作为数据框架，专注于将自定义数据源连接到LLM。两者的整合可能提供构建实际RAG应用程序的最佳性能和有效解决方案。在这个示例中，为了简单起见，我们将仅使用LangChain，搭配本地存储的PDF数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e18a0c-2811-41f0-ac14-b5fa9f7303b6",
   "metadata": {},
   "source": [
    "**安装依赖**：\n",
    "对于这个演示，我们将使用Gradio来构建聊天机器人的UI，使用文本生成推理框架进行模型服务。\n",
    "对于向量存储和相似性搜索，我们将使用FAISS。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93a502-ca6e-4a42-bfb7-3a3809423903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c74cf1-a9bb-4e1c-97ff-a4f5de3b6bec",
   "metadata": {},
   "source": [
    "# 1. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8edce-1fca-4470-bc19-c41f7b7e72e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "\n",
    "DATA_PATH = \"../data/\"\n",
    "DB_FAISS_PATH = \"../data/vectorstore/db_faiss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb465f-6375-48f7-ba7a-702a86475040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用PyPDFDirectoryLoader加载整个目录 (还可以使用 PyPDFLoader 加载单个文件)\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07719dc0-f0eb-455f-9722-f0c12a5dfd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551aacc-78fc-4638-b9d2-06eefb320aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0].page_content[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ee562-86ee-45bc-bc1e-9048bfee06b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将加载的文档分割成更小的块\n",
    "# RecursiveCharacterTextSplitter 是一种常见的拆分器，它将长文本片段拆分为较小的、具有语义意义的块。 \n",
    "# 其他splitters包括：\n",
    "# SpacyTextSplitter\n",
    "# NLTKTextSplitter\n",
    "# SentenceTransformersTokenTextSplitter\n",
    "# CharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(splits), splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51458b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a806-0c16-4131-85e9-fa5df1aadeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 HuggingFace 加载 Embedding 模型\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "                                   model_kwargs = {\"device\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6354c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将embedding存入向量数据库\n",
    "db = FAISS.from_documents(splits, embedding=embeddings)\n",
    "\n",
    "# 保存到本地\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4407f6-e30c-4fc4-af1f-3a6f218f35f3",
   "metadata": {},
   "source": [
    "# 2. 构建 Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78440d1-2b99-4632-abb8-11d2a49bc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from queue import Queue\n",
    "from typing import Any\n",
    "from langchain.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from anyio.from_thread import start_blocking_portal\n",
    "\n",
    "langchain.debug = True\n",
    "\n",
    "# 向量数据文件路径\n",
    "DB_FAISS_PATH = \"../data/vectorstore/db_faiss\"\n",
    "\n",
    "# LLaMA-2 7B host port\n",
    "LLAMA2_7B_HOSTPATH = \"http://localhost:9909/v1\"\n",
    "\n",
    "model_dict = {\n",
    "    \"7b-chat\" : LLAMA2_7B_HOSTPATH,\n",
    "}\n",
    "\n",
    "# 系统提示\n",
    "system_message = {\"role\" : \"system\",\n",
    "                  \"content\" : \"You are a helpful assistant.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407a85b-0460-4552-aa6e-e7baa2046ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载embedding\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"/slurm/home/yrd/shaolab/daiyizheng/resources/hf_weights/sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                   model_kwargs = {\"device\" : \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56754a1a-6a56-475c-9640-0bab4148ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 db\n",
    "\n",
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436a8f3-01d4-4117-861e-993efaf2e714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个模型服务实例\n",
    "# API : https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/openai.py#L144\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key = \"EMPTY\",\n",
    "    openai_api_base = LLAMA2_7B_HOSTPATH,\n",
    "    model_name = \"/slurm/home/yrd/shaolab/daiyizheng/resources/hf_weights/Qwen/Qwen1.5-7B\",\n",
    "    max_tokens = 300, \n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab95c1-67a8-4fa5-9832-2cf367563eb8",
   "metadata": {},
   "source": [
    "在构建基于检索的问答（RetrievalQA）链时，需要定义检索器（retriever）和模板。这两个组件在LangChain中扮演关键角色，用于处理和格式化查询以及生成答案。\n",
    "\n",
    "1. **定义检索器**：检索器的主要功能是在向量数据库中执行语义相似性搜索。当RetrievalQA被调用时，LangChain会根据用户的查询在向量数据库中进行搜索。这种搜索基于语义相似性，目的是找到与查询最相关的文档或数据片段。检索到的结果（上下文）随后被传递给Llama，以便对存储在向量数据库中的数据进行查询和生成答案。\n",
    "\n",
    "2. **定义模板**：模板则定义了将要发送到Llama进行生成的问题及其上下文的格式。Llama 2对于特殊标记有特殊的处理格式。在某些情况下，服务框架可能已经处理了这些特殊格式。如果没有，就需要编写自定义模板来正确处理这些特殊标记。模板的设计至关重要，因为它直接影响Llama 2如何理解问题和上下文，并据此生成回答。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fb0df-a3d2-4e71-a2a0-656fffad309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模板\n",
    "\n",
    "template = \"\"\"\n",
    "[INST]利用以下内容回答问题。如果没有提供上下文，请像人工智能助手一样回答。\n",
    "{context}\n",
    "问题: {question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89072f-c90d-4ef8-aa3a-8d7c3e87e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever 检索器\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs = {\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b44bc-6336-4b43-894d-6e76bcfd640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 chain\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template = template,\n",
    "            input_variables = [\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f073087-0c7a-4152-90dc-002c3436de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "result = qa_chain({\"query\": \"老年人糖脂代谢病主要有哪些病症? \"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dac163-119d-4a4e-9db5-98e579601ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此callback处理程序会将流式LLM响应放入队列中，以便gradio UI动态渲染。\n",
    "\n",
    "job_done = object()\n",
    "\n",
    "class MyStream(StreamingStdOutCallbackHandler):\n",
    "    def __init__(self, q) -> None:\n",
    "        self.q = q # 一个队列（Queue）对象\n",
    "        \n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.q.put(token) # 将新生成的token放入之前在构造函数中定义的队列self.q中\n",
    "        \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        # 将job_done对象放入队列self.q。这作为一个信号，表明语言模型已完成其工作。\n",
    "        self.q.put(job_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258b1fa-850e-44d6-b4c7-ae1b50b6938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio UI blocks\n",
    "\n",
    "import gradio as gr\n",
    "# 使用Gradio的Blocks接口创建一个新的UI布局\n",
    "with gr.Blocks() as demo:\n",
    "    # 定义UI布局\n",
    "    chatbot = gr.Chatbot(height=600)\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            with gr.Row():\n",
    "                # 创建一个下拉菜单，用于选择聊天机器人的模型。\n",
    "                model_selector = gr.Dropdown(\n",
    "                    list(model_dict.keys()),\n",
    "                    value = \"7b-chat\",\n",
    "                    label = \"Model\",\n",
    "                    info = \"Select the model\",\n",
    "                    interactive = True,\n",
    "                    scale = 1\n",
    "                )\n",
    "                # 创建一个数字输入框，用于设置生成文本的最大tokens数量\n",
    "                max_new_tokens_selector = gr.Number(\n",
    "                    value = 512,\n",
    "                    precision = 0,\n",
    "                    label = \"Max new tokens\",\n",
    "                    info = \"Adjust max_new_tokens\",\n",
    "                    interactive = True,\n",
    "                    minimum = 1,\n",
    "                    maximum = 1024,\n",
    "                    scale = 1\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                # 创建一个滑动条，用于调整生成文本的“温度”（创造性）,范围是 0 到 1。\n",
    "                temperature_selector = gr.Slider(\n",
    "                    value = 0.6,\n",
    "                    label = \"Temperature\",\n",
    "                    info = \"Range 0-1. Controls the creativity of the generated text.\",\n",
    "                    interactive = True,\n",
    "                    minimum = 0.01,\n",
    "                    maximum = 1,\n",
    "                    step = 0.01,\n",
    "                    scale = 1\n",
    "                )\n",
    "                # 创建另一个滑动条，用于调整“Top_p”参数，控制核采样。范围是 0.01 到 0.99。\n",
    "                top_p_selector = gr.Slider(\n",
    "                    value=0.9, \n",
    "                    label=\"Top_p\", \n",
    "                    info=\"Range 0-1. Nucleus sampling.\",\n",
    "                    interactive = True, \n",
    "                    minimum=0.01, \n",
    "                    maximum=0.99, \n",
    "                    step=0.01, \n",
    "                    scale=1\n",
    "                )\n",
    "        # 创建一个文本框，用于用户输入聊天机器人的提示语。\n",
    "        with gr.Column(scale=2):\n",
    "            # 用户输入区域\n",
    "            user_prompt_message = gr.Textbox(placeholder=\"Please add user prompt here\",\n",
    "                                             label=\"User prompt\")\n",
    "            with gr.Row():\n",
    "                # 创建一个按钮，用于清除聊天内容。\n",
    "                clear = gr.Button(\"Clear Conversation\", scale=2)\n",
    "                #  创建一个提交按钮，用于发送用户输入的提示语给聊天机器人。\n",
    "                submitBtn = gr.Button(\"Submit\", scale=8)\n",
    "                \n",
    "    # 使用 Gradio 的 State 创建一个状态变量，用于跨多个交互保持数据。\n",
    "    state = gr.State([])\n",
    "    \n",
    "    # 处理用户的消息\n",
    "    def user(user_prompt_message, history):\n",
    "        '''\n",
    "        user_prompt_message 是用户输入的消息\n",
    "        history 是之前的对话历史\n",
    "        '''\n",
    "        # 如果用户输入不为空\n",
    "        if user_prompt_message != \"\":\n",
    "            # 将用户消息添加到对话历史中\n",
    "            return history + [[user_prompt_message, None]]\n",
    "        else:\n",
    "            # 添加一条提示消息到对话历史，表示用户输入不能是空的。\n",
    "            return history + [[\"Invalid prompts - user prompt cannot be empty\", None]]\n",
    "        \n",
    "        \n",
    "    # 用于配置、发送提示、渲染生成等的聊天机器人逻辑\n",
    "    def bot(model_selector,\n",
    "            temperature_selector,\n",
    "            top_p_selector,\n",
    "            max_new_tokens_selector,\n",
    "            user_prompt_message,\n",
    "            history,\n",
    "            message_history\n",
    "           ):\n",
    "        # 初始化机器人的消息为空字符串\n",
    "        bot_message = \"\"\n",
    "        # 将历史记录中最后一条消息的机器人回复部分设置为空字符串\n",
    "        history[-1][1] = \"\"\n",
    "        # 创建一个字典，代表用户的角色和内容\n",
    "        dialog = [\n",
    "            {\"role\": \"user\", \"content\": user_prompt_message},\n",
    "        ]\n",
    "        # 将刚创建的对话条目添加到消息历史中\n",
    "        messages_history += dialog\n",
    "        # 创建一个队列用于处理异步任务的输出\n",
    "        q = Queue()\n",
    "        # 更新新的llama超参数\n",
    "        llm.openai_api_base = model_selector\n",
    "        llm.temperature = temperature_selector\n",
    "        llm.top_p = top_p_selector\n",
    "        llm.max_tokens = max_new_tokens_selector\n",
    "        \n",
    "        # 定义一个异步函数来运行LLM任务\n",
    "        async def task(prompt):\n",
    "            # 运行LLM任务，并将输出通过回调函数MyStream发送到队列 q。\n",
    "            ret = await qa_chain.run(prompt, callbacks=[MyStream(q)])\n",
    "            return ret\n",
    "        \n",
    "        # 使用上下文管理器来处理异步任务\n",
    "        with start_blocking_portal() as portal:\n",
    "            # 立即开始执行异步任务task\n",
    "            portal.start_task_soon(task, user_prompt_message)\n",
    "            while True:\n",
    "                # 持续从队列q中获取token\n",
    "                next_token = q.get(True)\n",
    "                # 检查是否接收到job_done信号，如果是，则将机器人的消息添加到消息历史中，\n",
    "                # 并返回更新后的历史记录和消息历史。\n",
    "                if next_token is job_done:\n",
    "                    messages_history += [{\"role\": \"assistant\",\n",
    "                                          \"content\": bot_message}]\n",
    "                    return history, messages_history\n",
    "                # 否则，将接收到的token添加到bot_message和history[-1][1]中\n",
    "                bot_message += next_token\n",
    "                history[-1][1] += next_token\n",
    "                yield history, messages_history\n",
    "                \n",
    "        def init_history(messages_history):\n",
    "            '''用于初始化消息历'''\n",
    "            messages_history = []\n",
    "            messages_history += [system_message]\n",
    "            return messages_history\n",
    "        \n",
    "        def input_cleanup():\n",
    "            '''用于清理输入，这里返回一个空字符串'''\n",
    "            return \"\"\n",
    "        \n",
    "        # 当用户在文本框中输入内容并按下回车键时，这个方法会被触发。\n",
    "        user_prompt_message.submit(\n",
    "            user, # user函数\n",
    "            [user_prompt_message, chatbot], # 输入参数包括user_prompt_message（用户输入的文本）和chatbot（聊天机器人组件）\n",
    "            [chatbot], # 输出参数是chatbot，这意味着user函数的结果将更新聊天机器人组件的状态。\n",
    "            queue = False, # 不将事件放入队列中异步处理，而是立即处理。\n",
    "        ).then(\n",
    "            # bot函数\n",
    "            bot, \n",
    "            # 输入参数包括各种控件的值\n",
    "            [model_selector, temperature_selector, top_p_selector, max_new_tokens_selector, user_prompt_message, chatbot, state],\n",
    "            # 输出参数是chatbot和state，这意味着bot函数的结果将更新聊天机器人组件的显示内容和聊天历史状态。\n",
    "            [chatbot, state]\n",
    "        ).then(\n",
    "            input_cleanup, # input_cleanup 函数\n",
    "            [], # 没有输入参数\n",
    "            [user_prompt_message], # 输出参数是user_prompt_message，用于清空用户输入文本框。\n",
    "            queue = False, # 不将事件放入队列中异步处理，而是立即处理。\n",
    "        )\n",
    "        # 当用户点击提交按钮时，这个方法会被触发。\n",
    "        submitBtn.click(\n",
    "            user,\n",
    "            [user_prompt_message, chatbot],\n",
    "            [chatbot],\n",
    "            queue = False # 不将事件放入队列中异步处理，而是立即处理。\n",
    "        ).then(\n",
    "            bot,\n",
    "            [model_selector, temperature_selector, top_p_selector, max_new_tokens_selector, user_prompt_message, chatbot, state],\n",
    "            [chatbot, state]\n",
    "        ).then(\n",
    "            input_cleanup,\n",
    "            [],\n",
    "            [user_prompt_message],\n",
    "            queue = False, # 不将事件放入队列中异步处理，而是立即处理。\n",
    "        )\n",
    "        \n",
    "        # 清除按钮触发\n",
    "        # 第1步: 当用户点击清除按钮时，这个方法会被触发。它执行一个简单的匿名函数（lambda: None），不进行任何操作，但会触发后续的 success 回调。\n",
    "        # 第2步: .success(init_history, [state], [state]): 成功执行点击事件后，调用 init_history 函数来初始化聊天历史。\n",
    "        # 输入参数是 state，用于重置聊天历史。\n",
    "        # 输出参数也是 state，更新后的聊天历史将反映在状态中。\n",
    "        clear.click(lambda: None, None, chatbot, queue=False).success(init_history, [state], [state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33fa7f-0734-4dcd-baae-e66b7a3e2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.queue().launch(server_name=\"0.0.0.0\", port=6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c825a8-26b5-4a8e-858b-dcd0f4076a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf\",\n",
    "                                                    model_kwargs={\"device\": \"cpu\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a018fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/slurm/home/yrd/shaolab/daiyizheng/.conda/envs/langechain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-25 17:25:24,376\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-25 17:25:24 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf)\n",
      "INFO 05-25 17:25:24 utils.py:660] Found nccl from library /slurm/home/yrd/shaolab/daiyizheng/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-25 17:25:25 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-25 17:25:25 selector.py:32] Using XFormers backend.\n",
      "INFO 05-25 17:25:41 model_runner.py:175] Loading model weights took 12.5523 GB\n",
      "INFO 05-25 17:25:43 gpu_executor.py:114] # GPU blocks: 7406, # CPU blocks: 512\n",
      "INFO 05-25 17:25:44 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-25 17:25:44 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-25 17:25:49 model_runner.py:1017] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(model=\"/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf\",\n",
    "           trust_remote_code=False,  # mandatory for hf models\n",
    "           max_new_tokens=300,\n",
    "           top_k=3,\n",
    "           top_p=0.95,\n",
    "           temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d0560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/slurm/home/yrd/shaolab/daiyizheng/.conda/envs/langechain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHello! I'm just an AI, and I'm here to help you with any questions or problems you might have. I'm a large language model trained on a wide range of texts, so I can provide information and answer questions on a variety of topics. Is there something specific you would like to know or talk about?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"你好，你是谁？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240a7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
