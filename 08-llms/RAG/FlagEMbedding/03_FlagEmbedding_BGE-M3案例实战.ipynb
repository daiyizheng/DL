{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGE-M3 应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/slurm/home/admin/.conda/envs/dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.786  0.4346]\n",
      " [0.4368 0.4753]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Dense Embedding 密集嵌入\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(model_name_or_path=\"/slurm/resources/weights/huggingface/BAAI/bge-m3\",\n",
    "                       use_fp16=True)\n",
    "\n",
    "sentences_1 = [\"这个周末我计划去海边度假，享受阳光和海浪\", \"最新研究表明，定期运动有助于提高工作效率和创造力\"]\n",
    "sentences_2 = [\"我期待的假期是在沙滩上，听着海浪声放松\", \"科技公司最近发布了一款新的智能手机，引起了广泛关注\"]\n",
    "\n",
    "embeddings_1 = model.encode(\n",
    "    sentences_1,\n",
    "    batch_size=12,\n",
    "    max_length=1024,\n",
    ")[\"dense_vecs\"]\n",
    "\n",
    "embeddings_2 = model.encode(\n",
    "    sentences_2,\n",
    "    batch_size=12,\n",
    "    max_length=1024,\n",
    ")[\"dense_vecs\"]\n",
    "\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sparse Embedding (Lexical Weight) 稀疏嵌入（词汇权重）\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from pprint import pprint\n",
    "\n",
    "model = BGEM3FlagModel(model_name_or_path=\"/slurm/resources/weights/huggingface/BAAI/bge-m3\",\n",
    "                       use_fp16=True)\n",
    "\n",
    "sentences_1 = [\"这个周末我计划去海边度假，享受阳光和海浪\", \"最新研究表明，定期运动有助于提高工作效率和创造力\"]\n",
    "sentences_2 = [\"周末去海边，听着海浪声方式，享受阳光\", \"科技公司最近发布了一款新的智能手机，引起了广泛关注\"]\n",
    "\n",
    "output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "\n",
    "output_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "\n",
    "# 查看每个token的权重\n",
    "pprint(model.convert_id_to_token(output_1[\"lexical_weights\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过词汇匹配计算得分\n",
    "\n",
    "lexical_scores = model.compute_lexical_matching_score(output_1[\"lexical_weights\"][0],\n",
    "                                                      output_2[\"lexical_weights\"][0])\n",
    "\n",
    "print(lexical_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.compute_lexical_matching_score(output_1[\"lexical_weights\"][0],\n",
    "                                           output_2[\"lexical_weights\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multi-Vector (ColBERT) 多向量\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(\"/slurm/resources/weights/huggingface/BAAI/bge-m3\", use_fp16=True)\n",
    "\n",
    "sentences_1 = [\"这个周末我计划去海边度假，享受阳光和海浪\", \"最新研究表明，定期运动有助于提高工作效率和创造力\"]\n",
    "sentences_2 = [\"周末去海边，听着海浪声方式，享受阳光\", \"科技公司最近发布了一款新的智能手机，引起了广泛关注\"]\n",
    "\n",
    "output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "\n",
    "output_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "\n",
    "print(model.colbert_score(output_1[\"colbert_vecs\"][0], output_2[\"colbert_vecs\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.colbert_score(output_1[\"colbert_vecs\"][0], output_2[\"colbert_vecs\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 计算文本对的分数 \n",
    "# 输入文本对列表，即可得到不同方法计算出的分数。\n",
    "\n",
    "sentences_1 = [\"什么是BGE M3？\", \"BM25的定义\"]\n",
    "sentences_2 = [\"BGE M3是一个支持密集检索、词汇匹配和多向量交互的嵌入模型。\", \n",
    "               \"BM25是一种词袋检索函数，它根据查询词汇在每个文档中出现的情况对一组文档进行排名\"]\n",
    "\n",
    "# 构建文本对\n",
    "sentence_pairs = [[i,j] for i in sentences_1 for j in sentences_2]\n",
    "\n",
    "print(sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model.compute_score(sentence_pairs,\n",
    "                          max_passage_length=128,\n",
    "                          # weights_for_different_modes(w) 用于执行加权和：\n",
    "                          # w[0]*dense_score + w[1]*sparse_score + w[2]*colbert_score\n",
    "                          weights_for_different_modes=[0.4, 0.2, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE-M3 微调\n",
    "训练数据应该是一个 jsonl 文件，其中每一行都是一个像这样的字典：  \n",
    "```shell\n",
    "{\"query\": str, \"pos\": List[str], \"neg\":List[str]}\n",
    "```  \n",
    "\n",
    "如果你想使用知识蒸馏，你的 jsonl 文件的每一行应该是这样的：  \n",
    "```shell\n",
    "{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"pos_scores\": List[float], \"neg_scores\": List[float]}\n",
    "```   \n",
    "\n",
    "`pos_scores`是正分数列表，其中`pos_scores[i]`是查询与教师模型中的`pos[i]`之间的分数。     \n",
    "`neg_scores`是负分数列表，其中`neg_scores[i]`是查询与教师模型中的`neg[i]`之间的分数。   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个简单的例子，说明如何基于BAAI/bge-m3进行统一微调（密集嵌入、稀疏嵌入和colbert）：  \n",
    "\n",
    "```shell\n",
    "# run_BGM-M3_finetune.sh\n",
    "\n",
    "torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.BGE_M3.run \\\n",
    "--output_dir /root/autodl-tmp/flagembedding/output_M3_finetuned_model \\\n",
    "--model_name_or_path /root/autodl-tmp/models/bge-m3 \\\n",
    "--train_data /root/autodl-tmp/samples/ \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 1 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 64 \\\n",
    "--passage_max_len 256 \\\n",
    "--train_group_size 2 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 10 \\\n",
    "--same_task_within_batch True \\\n",
    "--unified_finetuning True \\\n",
    "--use_self_distill True\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
