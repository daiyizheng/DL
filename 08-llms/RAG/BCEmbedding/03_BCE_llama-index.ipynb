{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama_index+\n",
    "```\n",
    "pip install llama_index==0.10.20   \n",
    "pip install llama-index-embeddings-huggingface==0.1.4   \n",
    "pip install llama-index-llms-huggingface==0.1.4   \n",
    "pip install llama-index-core==0.10.20.post2  \n",
    "   \n",
    "cd BCEmbedding  \n",
    "pip install -v -e . \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /slurm/home/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/slurm/home/admin/.conda/envs/dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /slurm/home/admin/.conda/envs/dl/lib/python3.11/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /slurm/home/admin/.conda/envs/dl/lib/python3.11/site-\n",
      "[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from BCEmbedding.tools.llama_index import BCERerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.retrievers import VectorIndexRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 embedding模型 和 reranker模型\n",
    "\n",
    "embed_args = {\"model_name\": '/slurm/resources/weights/huggingface/maidalun1020/bce-embedding-base_v1', \n",
    "              \"max_length\": 512,\n",
    "              \"embed_batch_size\": 32}\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(**embed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2024 03:20:24 - [INFO] -BCEmbedding.models.RerankerModel->>>    Loading from `/slurm/resources/weights/huggingface/maidalun1020/bce-reranker-base_v1`.\n",
      "07/27/2024 03:20:24 - [INFO] -BCEmbedding.models.RerankerModel->>>    Execute device: cuda;\t gpu num: 1;\t use fp16: False\n"
     ]
    }
   ],
   "source": [
    "# 初始化 embedding模型 和 reranker模型\n",
    "\n",
    "reranker_args = {\"model\": \"/slurm/resources/weights/huggingface/maidalun1020/bce-reranker-base_v1\",\n",
    "                  \"top_n\": 10,}\n",
    "\n",
    "reranker_model = BCERerank(**reranker_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding shape:  768\n",
      "passages_embeddings shape:  3\n"
     ]
    }
   ],
   "source": [
    "# 提取embeddings\n",
    "\n",
    "query = \"苹果\"\n",
    "\n",
    "passages = [\n",
    "    \"我喜欢苹果\",\n",
    "    \"我喜欢橘子\",\n",
    "    \"苹果和橘子都是水果\"\n",
    "]\n",
    "\n",
    "query_embedding = embed_model.get_query_embedding(query)\n",
    "print(\"query_embedding shape: \", len(query_embedding))\n",
    "\n",
    "passages_embeddings = embed_model.get_text_embedding_batch(passages)\n",
    "print(\"passages_embeddings shape: \", len(passages_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2024 03:40:02 - [INFO] -accelerate.utils.modeling->>>    We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:57<00:00, 29.45s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_template = PromptTemplate(\n",
    "    \"以下是描述一个任务的指令。\"\n",
    "    \"请根据用户输入的内容，一步一步地回答。\\n\\n\"\n",
    "    \"### 指令:\\n{query_str}\\n\\n### 回复:\"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=12000,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
    "    query_wrapper_prompt=qa_template,\n",
    "    tokenizer_name=\"/slurm/resources/weights/huggingface/Qwen/Qwen2-7B\",\n",
    "    model_name=\"/slurm/resources/weights/huggingface/Qwen/Qwen2-7B\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 512},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18834/3999105131.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"RAG-编年史.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='4c096d85-144f-473a-bd82-1b590f84d626', embedding=None, metadata={'page_label': '1', 'file_name': 'RAG-编年史.pdf', 'file_path': 'RAG-编年史.pdf', 'file_type': 'application/pdf', 'file_size': 1779723, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Abstract  \\n背景：\\nRAG是⼀种使⼤型语⾔模型（ LLMs）访问外部数据的流⾏范式。\\nRAG的实施⾯临诸多挑战，包括检索模型的有效整合、表示学习、数据多样性、计算效率优化等。\\n⽬的：\\n提出对巴⻄葡萄⽛语实施、优化和评估 RAG的良好实践。\\n通过简单的流⽔线进⾏推理和实验，回答有关哈利 ·波特书籍的问题。\\n⽅法：\\n使⽤ OpenAI的gpt-4、gpt-4-1106-preview 、gpt-3.5-turbo-1106 和Google的Gemini Pro ⽣成答案。\\n重点提升检索器的质量。\\n结果：\\n⽅法使得 MRR@10 相⽐基线提⾼了 35.4%。\\n经过优化，性能进⼀步提升了 2.4%。\\n结论：\\nRAG架构经过建议的增强后，相对得分从 57.88%提⾼到了 98.61%的最⼤值。\\n\\xa0\\n1 Introduction  \\n背景：\\nLLMs在AI应⽤中取得显著成绩，尤其在翻译、总结等任务中。\\n需要基于最新信息和外部数据提供答案的问题上存在挑战。\\n挑战：\\n实现 RAG技术⾯临新挑战，包括发展可靠检索器和确保检索⽂本相关性。\\nRAG领域发展：\\nRAG研究快速扩张，新论⽂介绍了不同的实现⽅式和技术改进。\\n这些发展给 AI实践者评估性能和适⽤性带来挑战。\\n研究内容：\\n针对巴⻄葡萄⽛语的 RAG实施的全⾯实验。\\n评估不同检索技术，探讨分块策略优化检索信息的整合。1. 论⽂名称： the chronicles of rag: the retriever, the chunk and the generator\\n           RAG编年史：检索器，⽂本块和⽣成器\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy\\n唐国梁Tommy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1500, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"万兴科技做什么的?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=3, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_by_embedding = vector_retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. 由于没有关于 ADA-002 的更多细节，⽂本中提到将这种⽅法仅称为密集检索器。\n",
      " \n",
      "5.1.3 Custom ADA-002  \n",
      "1. ⾃定义 ADA-002 ：\n",
      "采⽤了⾃定义 ADA-002 ⽅法，在第 5.1.2节中介绍的密集检索器配置中。\n",
      "嵌⼊⾃定义在提⾼整体表征中扮演了关键⻆⾊。\n",
      "2. 嵌⼊⾃定义技术：\n",
      "不仅限于 OpenAI的嵌⼊，适⽤于其他同类嵌⼊。\n",
      "有多种⽅法可以优化矩阵，其中之⼀是在未来⼯作中探索的多负例排名损失（ Multiple Negative  \n",
      "Ranking Loss ）。\n",
      "3. 微调阶段：\n",
      "需要两种类型的样本：正样本（标签为 1）和负样本（标签为 -1）。\n",
      "在数据集中，通常只有正样本，但可以通过简单的随机洗牌⽣成负样本。\n",
      "最终数据集包含约 400个例⼦，保持 1：3的正负样本⽐例。\n",
      "4. 影响性能的超参数：\n",
      "学习率、批量⼤⼩和投影矩阵的维度。\n",
      "ADA-002 模型有 1536个序列⻓度，投影矩阵的⼤⼩是 1536 x N ，其中 N可以是 1024、2048、4096。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2. 实施影响：\n",
      "选择的 BM25实现会影响效果。\n",
      "Pyserini的BM25实现包括预处理步骤，如词⼲提取和特定语⾔的停⽤词去除。\n",
      "为了⽐较，包括了使⽤ rank-bm25 的结果，这是⼀种没有预处理的基础实现，并且在 Python中⼴泛使\n",
      "⽤，集成到像 LangChain 和Llama-index 这样的库中。\n",
      "3. Pyserini BM25 的应⽤：\n",
      "在所有实验中都使⽤了 Pyserini BM25 实现，考虑   和  。\n",
      " \n",
      "5.1.2 ADA-002  \n",
      "1. ADA-002 架构：\n",
      "OpenAI没有公开 ADA-002 架构的详细信息。\n",
      "尽管如此，该模型在检索过程中被⽤作所展示的双编码器设计。\n",
      "2. 双编码器设计：\n",
      "为所有可⽤的块构建向量表示。\n",
      "每个输⼊查询在搜索时计算其嵌⼊。\n",
      "随后，使⽤余弦相似性评估查询和块之间的相似度。\n",
      "3. 图5说明：\n",
      "展示了双编码器架构，其中包括输⼊ 1的查询词标和输⼊ 2的块词标。\n",
      "两个输⼊在多层架构中各⾃编码，然后通过余弦相似度来评估它们之间的相似性。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "展示了混合 BM25-ADA-002 ⽅法的检索器⽐较。\n",
      "其中包括平均倒数排名（ MRR）和召回率（ R@k）。\n",
      "实验中，只有 Pyserini的BM25被测试为稀疏检索器，⽽ ADA-002 和⾃定义 ADA-002 被测试为密集检索\n",
      "器。\n",
      "提供了最佳结果的混合组合是使⽤ BM25和⾃定义 ADA-002 。\n",
      " \n",
      "5.1.5 Reranker 重排  \n",
      "1. 重排器的基本概念：\n",
      "多阶段排名的基础是将⽂档排名分为⼀系列阶段，每个后续阶段重新评估并排名前⼀阶段传递来的候选\n",
      "集。\n",
      "2. 多阶段排名：\n",
      "图9展示了⼀个多阶段管道，其中 Pyserini BM25 负责第⼀阶段，然后候选块由重排器重新评估。\n",
      "重排后的列表称为检索到的块，由   个块组成最终结果。\n",
      "3. 重排器的使⽤：\n",
      "通常使⽤基于 Transformer 的模型作为重排器，利⽤它们捕捉⽂档和查询中复杂关系和上下⽂信息的能\n",
      "⼒来提⾼信息检索系统的效果。\n",
      "最初在多阶段排名框架内使⽤ Transformer 的研究提出了 monoBERT ，将排名过程转换为相关性分类问\n",
      "题。\n",
      "4. monoBERT 与monoT5 ：\n",
      "monoBERT 通过计算   来对⽂本进⾏排序，其中   是查询，  表示⽂档。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in retrieval_by_embedding:\n",
    "    print(item.text + \"\\n\\n\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "retrieval_by_reranker = reranker_model.postprocess_nodes(retrieval_by_embedding, query_str=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 实施影响：\n",
      "选择的 BM25实现会影响效果。\n",
      "Pyserini的BM25实现包括预处理步骤，如词⼲提取和特定语⾔的停⽤词去除。\n",
      "为了⽐较，包括了使⽤ rank-bm25 的结果，这是⼀种没有预处理的基础实现，并且在 Python中⼴泛使\n",
      "⽤，集成到像 LangChain 和Llama-index 这样的库中。\n",
      "3. Pyserini BM25 的应⽤：\n",
      "在所有实验中都使⽤了 Pyserini BM25 实现，考虑   和  。\n",
      " \n",
      "5.1.2 ADA-002  \n",
      "1. ADA-002 架构：\n",
      "OpenAI没有公开 ADA-002 架构的详细信息。\n",
      "尽管如此，该模型在检索过程中被⽤作所展示的双编码器设计。\n",
      "2. 双编码器设计：\n",
      "为所有可⽤的块构建向量表示。\n",
      "每个输⼊查询在搜索时计算其嵌⼊。\n",
      "随后，使⽤余弦相似性评估查询和块之间的相似度。\n",
      "3. 图5说明：\n",
      "展示了双编码器架构，其中包括输⼊ 1的查询词标和输⼊ 2的块词标。\n",
      "两个输⼊在多层架构中各⾃编码，然后通过余弦相似度来评估它们之间的相似性。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "展示了混合 BM25-ADA-002 ⽅法的检索器⽐较。\n",
      "其中包括平均倒数排名（ MRR）和召回率（ R@k）。\n",
      "实验中，只有 Pyserini的BM25被测试为稀疏检索器，⽽ ADA-002 和⾃定义 ADA-002 被测试为密集检索\n",
      "器。\n",
      "提供了最佳结果的混合组合是使⽤ BM25和⾃定义 ADA-002 。\n",
      " \n",
      "5.1.5 Reranker 重排  \n",
      "1. 重排器的基本概念：\n",
      "多阶段排名的基础是将⽂档排名分为⼀系列阶段，每个后续阶段重新评估并排名前⼀阶段传递来的候选\n",
      "集。\n",
      "2. 多阶段排名：\n",
      "图9展示了⼀个多阶段管道，其中 Pyserini BM25 负责第⼀阶段，然后候选块由重排器重新评估。\n",
      "重排后的列表称为检索到的块，由   个块组成最终结果。\n",
      "3. 重排器的使⽤：\n",
      "通常使⽤基于 Transformer 的模型作为重排器，利⽤它们捕捉⽂档和查询中复杂关系和上下⽂信息的能\n",
      "⼒来提⾼信息检索系统的效果。\n",
      "最初在多阶段排名框架内使⽤ Transformer 的研究提出了 monoBERT ，将排名过程转换为相关性分类问\n",
      "题。\n",
      "4. monoBERT 与monoT5 ：\n",
      "monoBERT 通过计算   来对⽂本进⾏排序，其中   是查询，  表示⽂档。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4. 由于没有关于 ADA-002 的更多细节，⽂本中提到将这种⽅法仅称为密集检索器。\n",
      " \n",
      "5.1.3 Custom ADA-002  \n",
      "1. ⾃定义 ADA-002 ：\n",
      "采⽤了⾃定义 ADA-002 ⽅法，在第 5.1.2节中介绍的密集检索器配置中。\n",
      "嵌⼊⾃定义在提⾼整体表征中扮演了关键⻆⾊。\n",
      "2. 嵌⼊⾃定义技术：\n",
      "不仅限于 OpenAI的嵌⼊，适⽤于其他同类嵌⼊。\n",
      "有多种⽅法可以优化矩阵，其中之⼀是在未来⼯作中探索的多负例排名损失（ Multiple Negative  \n",
      "Ranking Loss ）。\n",
      "3. 微调阶段：\n",
      "需要两种类型的样本：正样本（标签为 1）和负样本（标签为 -1）。\n",
      "在数据集中，通常只有正样本，但可以通过简单的随机洗牌⽣成负样本。\n",
      "最终数据集包含约 400个例⼦，保持 1：3的正负样本⽐例。\n",
      "4. 影响性能的超参数：\n",
      "学习率、批量⼤⼩和投影矩阵的维度。\n",
      "ADA-002 模型有 1536个序列⻓度，投影矩阵的⼤⼩是 1536 x N ，其中 N可以是 1024、2048、4096。\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "唐国梁Tommy\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in retrieval_by_reranker:\n",
    "    print(item.text + \"\\n\\n\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询引擎：方式-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/slurm/home/admin/.conda/envs/dl/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.25` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(node_postprocessors=[reranker_model])\n",
    "query_response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "根据提供的上下文信息，万兴科技是一家专注于开发和销售数字创意软件的公司。该公司提供一系列软件产品，包括视频编辑、音频编辑、图像编辑、文档编辑、PDF编辑、动画制作、图形设计、音乐制作和数字营销等。这些软件产品旨在帮助用户轻松地创建、编辑和分享数字内容。\n"
     ]
    }
   ],
   "source": [
    "print(str(query_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询引擎：方式-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(vector_retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "根据提供的上下文信息，万兴科技是一家专注于开发和销售数字创意软件的公司。该公司提供一系列软件产品，包括视频编辑、音频编辑、图像编辑、文档编辑、PDF编辑、动画制作、图形设计、音乐制作和数字营销等。这些软件产品旨在帮助用户轻松地创建、编辑和分享数字内容。\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
