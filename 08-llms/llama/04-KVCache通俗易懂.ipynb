{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kvcache原理、参数量、代码详解\n",
    "kvcache一句话来说就是把每个token在过Transformer时乘以W_K,W_V这俩参数矩阵的结果缓存下来。训练的时候不需要保存。推理解码生成时都是自回归auto-regressive的方式，也就是每次生成一个token，都要依赖之前token的结果。如果没生成一个token的时候乘以W_K,W_V这俩参数矩阵要对所有token都算一遍，代价非常大，所以缓存起来就叫kvcache。\n",
    "举个例子，假如prompt=“The largest city of China is”，输入是6个tokens，返回是\"Shang Hai\"这两个tokens。整个生成过程如下：\n",
    "\n",
    "- 当生成\"Shang\"之前，kvcache把输入6个tokens都乘以$W_K$,$W_V$这两参数矩阵，也就是缓存了6个kv。这时候过self-attention+采样方案(greedy、beam、top k、top p等)，得到\"Shang\"这个token\n",
    "- 那么问题就又来了，生成\"Shang\"这个token时，感觉是“The largest city of China is”这6个tokens的query都用了，但是生成\"Hai\"这个token时，只依赖了\"Shang\"这个token的query嘛？这个问题其实是没有的，每个token的生成都只依赖前一个Q和之前所有的KV！！！借用https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/#further-optimizing-subsequent-iterations里的下图来看\n",
    "\n",
    "**为什么有kv cache，没有q cache？**    \n",
    "一句话来说q cache并没有用。展开解释就是整个scaled dot production公式$\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$，每次新多一个Q中的token时，用新多的这个token和所有tokens的K、V去乘就好了，Q cache就成多余的了。再拿刚才例子强调一下，当生成\"Shang\"之后，再次过transformer的token只有\"Shang\"这一个token，而不是整个“The largest city of China is Shang”句子\n",
    "\n",
    "那么问题就又来了，生成\"Shang\"这个token时，感觉是“The largest city of China is”这6个tokens的query都用了，但是生成\"Hai\"这个token时，只依赖了\"Shang\"这个token的query嘛？这个问题其实是没有的，每个token的生成都只依赖前一个Q和之前所有的KV！！！\n",
    "\n",
    "- 训练的时候’Quant’,‘um’,'_mechan’的下一个token在矩阵乘法时对应的是蓝框，被mask没了\n",
    "- 推理的时候在给定’Quant’,‘um’,'_mechan’的时候，已有的序列长度是3，矩阵乘法是由图中红框决定的，刚好和未来没读到的蓝框token没有任何关系。同时，‘_mechan’的下一个token只和’_mechan’的Q有关，和’Quant’,'um’的Q是无关的！！！所以每个token的生成都只依赖前一个Q和之前所有的KV，这也是kvcache能work下去的基础！\n",
    "\n",
    "<img src=\"../images/07.png\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入序列的长度为s，输出序列的长度为n，以float16来保存KV cache，那么KV cache的峰值显存占用大小为$b*(s+n)h*l*2*2 =4lh(s +n)b∗(s+n)h∗l∗2∗2=4lh(s+n)$。这里第一个2表示K/V cache，第2个2表示float16占2个bytes。\n",
    "以GPT3为例，对比KV cache与模型参数占用显存的大小。GPT3模型占用显存大小为350GB。假设批次大小b=64，输入序列长度 =512，输出序列长度n =32，则KV cache占用显存大约为$4lh(s+n)$= 164,282,499,072bytes约等于164GB，大约是模型参数显存的0.5倍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvcache代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ...\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "        # hidden_states对应query\n",
    "        # encoder_hidden_states对应key和value\n",
    "        if encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"q_attn\"):\n",
    "                raise ValueError(\n",
    "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                )\n",
    "\n",
    "            query = self.q_attn(hidden_states)\n",
    "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            # 切成多头以后，shape是[bs, num_heads, seq_length, head_dim]，所以dim=-2就是seq_length对应的那个维度，拼一下就有了新的kvcache\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        if self.reorder_and_upcast_attn:\n",
    "            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "        else:\n",
    "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        # 最后输出的present就是当前的kvcache\n",
    "        return outputs  # a, present, (attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以GPT2为例，分析下kvcache的传递链路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "text = \"The largest city of China is\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "forward_output = model(**encoded_input)\n",
    "# forward_output['logits'].shape\n",
    "# len(forward_output['past_key_values'])\n",
    "generate_output = model.generate(**encoded_input)\n",
    "generate_text = tokenizer.decode(generate_output[0])\n",
    "print(generate_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
