{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 类定义\n",
    "\n",
    "## 采样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于解析和生成JSON数据\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "# 用于处理文件路径的类，使得代码能够以面向对象的方式处理文件和目录路径。\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FairScale是一个PyTorch扩展库，用于进行大规模训练\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    # 函数用于初始化模型并行环境，模型并行指的是将一个大型模型分割成多个部分，然后在不同的处理器或机器上并行执行。\n",
    "    initialize_model_parallel,\n",
    "    # 函数用于获取当前设备在模型并行中的等级，在模型并行中，每个进程通常负责模型的一个部分。这个“排名”标识了进程处理的是模型的哪一部分。\n",
    "    get_model_parallel_rank,\n",
    "    # 函数则是用于检查模型并行环境是否已经初始化\n",
    "    model_parallel_is_initialized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelArgs, Transformer\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Literal : 用于指定一个变量只能取固定的字面值\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "# TypedDict : 用于为字典指定预期的键和对应值的类型\n",
    "class Message(TypedDict):\n",
    "    role: Role # 角色\n",
    "    content: str # 消息的内容\n",
    "    \n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    '''这个字典主要用于存储文本生成任务的结果和相关信息'''\n",
    "    # 将total参数设置为False，这个TypedDict被指定为非严格模式，这意味着不是所有的键都是必需的。\n",
    "    generation: str # 生成的文本\n",
    "    tokens: List[str] # 字符串列表，是生成文本的单词标记列表。此字段不是必需的。\n",
    "    logprobs: List[float] # 浮点数列表，表示生成每个单词标记的对数概率。此字段也不是必需的。\n",
    "    \n",
    "    \n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message # Message类，表示生成的聊天信息\n",
    "    tokens: List[str] # 字符串列表，是生成文本的单词标记列表。此字段不是必需的。\n",
    "    logprobs: List[str] # 浮点数列表，表示生成每个单词标记的对数概率。此字段也不是必需的。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义为Message类型的列表。每个Message都是一个字典，包含role和content字段。\n",
    "\n",
    "Dialog = List[Message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别表示一个指令的开始和结束标记\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "# 分别表示一个系统消息的开始和结束标记\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认的系统提示\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    '''这种采样方法是为了在文本生成过程中增加多样性，防止模型仅生成概率最高的词，同时也能防止生成概率极低的词。'''\n",
    "    \n",
    "    # 将概率probs降序排序，并返回排序后的值probs_sort和对应的索引probs_idx\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    # 计算累积的概率\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    # 标记出累加和大于阈值p的元素\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    # 用0填充probs_sort中所有mask为True的位置，这意味着所有累积概率超过p的词都不会被考虑。\n",
    "    probs_sort[mask] = 0.0\n",
    "    # 对概率进行归一化，使得剩下的token的概率之和为1。\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    # 从修改后的probs_sort中随机抽样，得到下一个token的索引，torch.multinomial函数可以从给定的多项分布中抽取样本。\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    # 根据索引从排序之前的概率分布中找到对应的token\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import　torch\n",
    "random_seed = 200\n",
    "torch.manual_seed(random_seed)\n",
    "input = torch.randint(0, 100, (2, 3, 4))\n",
    "print(\"input:\")\n",
    "print(input)\n",
    "\n",
    "index = torch.randint(0, 2, (2, 1, 2))\n",
    "print(\"index:\")\n",
    "print(index)\n",
    "\n",
    "output = input.gather(0, index)\n",
    "print(\"output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama:\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # 构建 Llama 类的实例\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str, # checkpoint的文件夹路径 \n",
    "        tokenizer_path: str, # 分词器模型的文件路径\n",
    "        max_seq_len: int, # 输入文本的最大序列长度\n",
    "        max_batch_size: int, # 推理时的最大批处理大小\n",
    "        model_parallel_size: Optional[int] = None, # 模型并行处理的进程数，是可选参数。\n",
    "    ) -> \"Llama\":\n",
    "        # 1. 检查分布式环境是否已初始化\n",
    "        if not torch.distributed.is_initialized():\n",
    "            # 如果没有，它将使用NCCL后端初始化分布式环境。\n",
    "            # NCCL（Nvidia Collective Communications Library）是一个用于GPU之间通信的库，主要用于深度学习的训练中。\n",
    "            torch.distributed.init_process_group(\"nccl\")\n",
    "        \n",
    "        # 2. 检查模型并行环境是否已初始化\n",
    "        if not model_parallel_is_initialized():\n",
    "            # 初始化模型并行环境\n",
    "            if model_parallel_size is None:\n",
    "                # 从环境变量中获取 WORLD_SIZE 的值来确定模型并行的大小。如果未设置，则默认为 1。\n",
    "                model_parallel_size = int(os.envirion.get(\"WORLD_SIZE\", 1))\n",
    "            # 使用指定的模型并行大小来初始化模型并行\n",
    "            initialize_model_parallel(model_parallel_size)\n",
    "            \n",
    "        # 获取local rank，用于设置当前进程使用的GPU。\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        \n",
    "        # 在分布式训练中，每个进程都会有一个local rank，它表示这个进程在当前节点中的rank。\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        \n",
    "        # 确保所有进程的一致性，方法设置了一个固定的随机种子 1\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        # 在多GPU环境中，这段代码确保只有主进程（local_rank == 0）会输出到标准输出，\n",
    "        # 其他进程的输出被重定向到空设备，即不显示。\n",
    "        if local_rank > 0:\n",
    "            sys.stdout = open(os.devnull, \"w\")\n",
    "        \n",
    "        # 用于后续计算加载模型花费的时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 搜索包含所有的.pth文件（PyTorch模型文件）的目录，确保至少找到一个这样的文件。\n",
    "        # 这些文件是分布式训练结果，每个文件代表一个训练的部分。\n",
    "        checkpoints = sorted(Path(ckpt_dir)).glob(\"*.pth\")\n",
    "        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n",
    "        \n",
    "        # 检查模型并行的大小是否等于找到的.pth文件的数量。如果不等于，它将抛出一个异常，\n",
    "        # 因为每一个模型并行的部分需要一个对应的.pth文件。\n",
    "        assert model_parallel_size == len(checkpoints), \\\n",
    "        f\"Loading a checkpoint for MP={len(checkpoints)} , but world size is {model_parallel_size}\"\n",
    "        \n",
    "        # 根据模型并行的等级加载对应的.pth文件，并行等级决定了加载哪一个.pth文件。\n",
    "        ckpt_path = checkpoints[get_model_parallel_rank()]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        \n",
    "        # 打开并加载一个名为\"params.json\"的文件，这个文件包含了模型的参数\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "            \n",
    "        # 创建了一个ModelArgs实例\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params, # 其他从JSON文件中读取的参数\n",
    "        )\n",
    "        \n",
    "        # 初始化了一个Tokenizer实例\n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        \n",
    "        # 将分词器的词汇表大小设置为模型的参数\n",
    "        model_args.vocab_size = tokenizer.n_words\n",
    "        \n",
    "        # 代码将默认的Tensor类型设置为半精度浮点数（HalfTensor）。\n",
    "        # 这样做的目的是减少内存消耗并加速计算，但可能会导致计算的精度略有下降。\n",
    "        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        \n",
    "        # 创建了一个Transformer模型的实例\n",
    "        model = Transformer(model_args)\n",
    "        \n",
    "        # 从之前加载的模型参数检查点（checkpoint）中加载参数\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # 创建了一个Llama类的实例，传入了模型和分词器作为参数，并返回这个实例。\n",
    "        return Llama(model, tokenizer)\n",
    "    '''知识补充：\n",
    "    模型并行涉及多个checkpoint文件的原因是什么？\n",
    "    （1）分割模型的不同部分： 在模型并行设置中，一个大型的模型被分割成多个较小的部分。每个部分可以在不同的处理单元（例如GPU）上运行。\n",
    "                          这意味着模型的不同部分可以并行处理，提高了计算效率和规模。\n",
    "\n",
    "    （2）独立保存每部分的状态：由于模型被分割成多个部分，每个部分的状态（权重和偏差）通常单独保存在不同的检查点文件中。\n",
    "                          这样做的主要原因是简化模型的保存和加载过程，尤其是在大规模分布式训练环境中。\n",
    "\n",
    "    （3）加载时的并行恢复： 当加载模型用于推理或进一步训练时，这些检查点文件可以被并行地加载到相应的处理单元上。\n",
    "                          这样可以快速恢复模型的完整状态，同时保持模型部分的并行性。\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    @torch.inference_mode() # @torch.inference_mode() 是PyTorch的一个装饰器，用于优化模型推理（inference）时的性能。\n",
    "    def generate(\n",
    "        self,\n",
    "        # 输入的提示token，是一个二维列表，每个元素是一个token列表。\n",
    "        prompt_tokens: List[List[int]],\n",
    "        # 最大的生成长度\n",
    "        max_gen_len: int, \n",
    "        # 温度\n",
    "        temperature: float = 0.6, \n",
    "        # 用于控制生成新token时采样的方式，只有概率在前top_p的token才会被考虑。\n",
    "        top_p: float = 0.9, \n",
    "        # 如果为True，返回每个生成token的对数概率。\n",
    "        logprobs: bool = False, \n",
    "        # 如果为True，输入的提示token也会被包含在生成的结果中。\n",
    "        echo: bool = False, \n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        # 检查输入的批次大小是否超过了模型参数的最大批次大小\n",
    "        assert bsz < params.max_batch_size, (bsz, params.max_batch_size)\n",
    "        \n",
    "        # 首先获取了输入提示tokens的最小和最大长度，然后确保最大长度不超过模型参数的最大序列长度。\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len < params.max_seq_len\n",
    "        \n",
    "        # 总长度不超过最大序列长度\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "        \n",
    "        # 使用填充tokens初始化一个用于存储生成tokens的张量\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        \n",
    "        # 对于每一个输入的提示token，将其复制到tokens张量的对应行中\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "            \n",
    "        # 如果logprobs参数为True，还会创建一个与tokens张量大小相同的token_logprobs张量，\n",
    "        # 用于存储生成的每个token的对话概率。\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zero_like(tokens, dtype=torch.float)\n",
    "        \n",
    "        # 在生成文本时每一步对模型进行前向传播，并可能计算每个生成的单词的对数概率。\n",
    "        # 表示在前向传播过程中模型应该从哪个位置开始考虑输入token\n",
    "        prev_pos = 0\n",
    "        \n",
    "        # 初始化一个布尔型的张量eos_reached，长度等于批次大小（bsz）。\n",
    "        # 这个张量的每个元素都表示一个输入样本是否已经生成了结束token（EOS）。\n",
    "        # 初始时，所有元素都被设置为False，表示还没有生成EOS。\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        \n",
    "        # 创建一个掩码input_text_mask，用于标记输入tokens中哪些位置的token是实际的输入token，哪些是padding token。\n",
    "        input_text_mask = tokens != pad_id\n",
    "        \n",
    "        # cur_pos 表示当前要生成的token的位置\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            # 调用模型的前向传播函数，对输入tokens的一部分进行处理，从位置prev_pos到位置cur_pos。\n",
    "            # 这会返回一个logits张量，该张量包含了模型对每个可能的下一个token的原始预测。\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            \n",
    "            # 检查是否需要计算每个生成的token的对数概率\n",
    "            if logprobs:\n",
    "                # F.cross_entropy是PyTorch提供的交叉熵损失函数，它需要两个输入：预测的概率（即logits）和实际的目标tokens。\n",
    "                # 通过计算交叉熵，可以得到模型对每个token的预测概率的负对数值。负对数概率越低，表示模型对该token的预测越有信心。\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    # input是预测输入，这里是logits的转置。cross_entropy需要的logits形状是 (N, C, *)，\n",
    "                    # 其中N是批量大小，C是类别数，*是任何其他维度。在这里，由于logits最初的形状是(N, *, C)，\n",
    "                    input = logits.transpose(1, 2),\n",
    "                    target = tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    # reduction参数决定了如何将每个样本的损失整合起来。设置为none表示不对损失进行整合，\n",
    "                    # 而是返回一个和目标大小一致的张量，这样就可以得到每个token的单独损失。\n",
    "                    reduction=\"none\",\n",
    "                    # 设置pad_id为忽略的目标值，这样pad token的损失就不会被计算进去。\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "                \n",
    "            if temperature > 0:\n",
    "                # 使用温度对logits进行缩放，使生成的分布变得更锐利（温度低）或更平均（温度高）。\n",
    "                # softmax函数用于将logits转换为概率分布。\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                \n",
    "                # 使用sample_top_p函数从调整过的概率分布中抽样生成下一个token。\n",
    "                # 这个函数首先按概率降序排序所有可能的token，\n",
    "                # 然后选择一个概率累计达到top_p的最小集合，最后在这个集合中随机选择一个token。\n",
    "                # 这种方法叫做\"nucleus sampling\"，它是一种平衡生成质量和多样性的方法。\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # 如果温度为0，则直接选择具有最高logits的token作为下一个token。\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "                \n",
    "            # reshape为一维向量\n",
    "            next_token = next_token.reshape(-1)\n",
    "            \n",
    "            # 使用一个条件来决定是否应该使用输入tokens的当前位置的值，还是新生成的next_token。\n",
    "            # input_text_mask是一个布尔值的mask，其中对于每一个位置，如果其值为True，\n",
    "            # 表示这个位置在输入的文本中；如果为False，表示这个位置是需要生成的位置。\n",
    "            # torch.where根据条件从两个选项中选择一个值。在我们的例子中，如果input_text_mask在cur_pos位置的值为True，\n",
    "            # 我们会使用tokens[:, cur_pos]的值，否则使用新生成的next_token。\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            \n",
    "            # 将新生成的next_token放入tokens张量的当前位置\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            \n",
    "            # 检查是否已经生成了EOS token\n",
    "            # 如果当前位置不是输入文本的一部分（~input_text_mask[:, cur_pos]返回True）\n",
    "            # 并且生成的next_token 是EOS token（next_token == self.tokenizer.eos_id 返回True），\n",
    "            # 那么eos_reached将被设为True。\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            '''知识补充: \n",
    "            按位非运算符 ~，代码将输入文本的掩码值反转，如果当前位置是输入文本的一部分，结果将为False，反之则为True。\n",
    "            通过位或运算符 |= 更新eos_reached变量。如果上述条件为True，则eos_reached将被设置为True。否则，如果eos_reached已经是 True，它将保持不变。\n",
    "            '''\n",
    "            \n",
    "            # 将当前位置cur_pos保存到变量prev_pos中，以便在下一个循环中使用\n",
    "            prev_pos = cur_pos\n",
    "            \n",
    "            # 如果所有的输入序列都已经生成了EOS token，那么就提前结束循环。\n",
    "            # 在实际使用中，这可以帮助我们提高效率，因为一旦所有的序列都已经完成生成，就没有必要继续计算了。\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "                \n",
    "            if logprobs:\n",
    "                # 从PyTorch tensor转换为Python列表\n",
    "                token_logprobs = token_logprobs.tolist()\n",
    "                \n",
    "            # 遍历，对每个生成的tokens进行处理\n",
    "            out_tokens, out_logprobs = [], []\n",
    "            for i, toks in enumerate(tokens.tolist()):\n",
    "                # 如果echo为True，那么就从0开始，否则从输入prompt的长度开始。\n",
    "                start = 0 if echo else len(prompt_tokens[i])\n",
    "                # 截取了生成的tokens到指定的长度max_gen_len\n",
    "                toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "                probs = None\n",
    "                \n",
    "                if logprobs:\n",
    "                    # 截取了对应的logprobs到指定的长度\n",
    "                    probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "                    \n",
    "                # 是否存在eos_id（句子结束标记）\n",
    "                if self.tokenizer.eos_id in toks:\n",
    "                    # 获取了eos_id的位置\n",
    "                    eos_idx = toks.index(self.tokenizer.eos_id)\n",
    "                    # 截断\n",
    "                    toks = toks[:eos_idx]\n",
    "                    probs = probs[:eos_idx] if logprobs else None\n",
    "                \n",
    "                # 将处理后的tokens和logprobs添加到结果列表中\n",
    "                out_tokens.append(toks)\n",
    "                out_logprobs.append(probs)\n",
    "                \n",
    "            return (out_tokens, out_logprobs if logprobs else None)\n",
    "        \n",
    "        \n",
    "        # 基于输入的提示文本进行文本生成\n",
    "        def text_completion(\n",
    "            self,\n",
    "            prompts: List[str], # 一个字符串列表，每个字符串都是一个输入的提示文本。\n",
    "            temperature: float = 0.6, \n",
    "            top_p: float = 0.9, # 用于控制生成新token时采样的方式，只有概率在前top_p的token才会被考虑。\n",
    "            max_gen_len: Optional[int] = None, # 控制生成的文本的最大长度\n",
    "            logprobs: bool = False, # 如果为True，返回每个生成token的log概率。\n",
    "            echo: bool = False, # 如果为True，输入的提示token也会被包含在生成的结果中。\n",
    "        ) -> List[CompletionPrediction]:\n",
    "            if max_gen_len is None:\n",
    "                # 设置为模型参数的最大序列长度减1\n",
    "                max_gen_len = self.model.params.max_seq_len - 1\n",
    "            \n",
    "            # 将输入的提示文本编码成tokens，同时添加一个bos（begin of sequence）token在每个序列的开始，\n",
    "            # 并且不在每个序列的结束添加eos（end of sequence）token。\n",
    "            prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "            \n",
    "            # 生成文本，返回生成的tokens和相应的log概率\n",
    "            generation_tokens, generation_logprobs = self.generate(\n",
    "                prompt_tokens = prompt_tokens,\n",
    "                max_gen_len = max_gen_len,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                logprobs = logprobs,\n",
    "                echo = echo,\n",
    "            )\n",
    "            \n",
    "            if logprobs:\n",
    "                # 需要返回每个生成token的log概率\n",
    "                return [\n",
    "                    {\n",
    "                        \"generation\": self.tokenizer.decode(t), # 解码后的生成文本\n",
    "                        \"tokens\": [self.tokenizer.decode(x) for x in t], # 生成文本中的每个token解码后的形式\n",
    "                        \"logprobs\": logprobs_i, # 每个生成token的log概率\n",
    "                    }\n",
    "                    for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "                ]\n",
    "            \n",
    "            # 如果logprobs为False，则返回一个字典列表。\n",
    "            return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def chat_completion(\n",
    "            self,\n",
    "            # 对话信息的列表，每个对话Dialog是一个字典列表，表示对话中的一条消息。\n",
    "            # 对话中的每条消息都有一个角色（role）和内容（content）。\n",
    "            dialogs: List[Dialog],\n",
    "            temperature: float = 0.6,\n",
    "            top_p: float = 0.9,\n",
    "            max_gen_len: Optional[int] = None,\n",
    "            logprobs: bool = False,\n",
    "        ) -> List[ChatPrediction]:\n",
    "            \n",
    "            if max_gen_len is None:\n",
    "                # 设置为模型参数的最大序列长度减1\n",
    "                max_gen_len = self.model.params_max_seq_len - 1\n",
    "                \n",
    "            prompt_tokens = []\n",
    "            for dialog in dialogs:\n",
    "                # 确保每一个对话都是以系统提示开始的，并且消息会以user/assistant/user/assistant的顺序交替进行\n",
    "                if dialog[0][\"role\"] != \"system\":\n",
    "                    dialog = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "                        }\n",
    "                    ] + dialog\n",
    "                    \n",
    "                # 将对话中的前两条消息组合成一条，第一条消息的内容被包含在B_SYS和E_SYS标记中，\n",
    "                # 并与第二条消息的内容组合起来。这是因为在该模型中，系统提示和用户的首条消息被视为一个整体，\n",
    "                # 用于生成模型的第一个回复。\n",
    "                dialog = [\n",
    "                    {\n",
    "                        \"role\": dialog[1][\"role\"],\n",
    "                        \"content\": B_SYS + dialog[0][\"content\"] + E_SYS + dialog[1][\"content\"],\n",
    "                    }\n",
    "                ] + dialog[2:]\n",
    "                \n",
    "                # 使用assert来确保在对话中的消息是以用户和助手的顺序交替进行的\n",
    "                assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and \\\n",
    "                       all([msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]), \\\n",
    "                       (\"model only supports 'system', 'user' and 'assistant' roles, \"\n",
    "                        \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\")\n",
    "                        \n",
    "                \n",
    "                # ① 对于每一对prompt和answer，将其格式化为模型所需要的格式，这种格式包含B_INST（实例开始标记）和E_INST（实例结束标记）。\n",
    "                # ② 对格式化的字符串进行编码，使其变为一串整数列表。\n",
    "                # ③ 使用zip函数将prmpt和answer对应起来，这样可以确保每一对prompt和answer都是一组。\n",
    "                # ④ 将对每一对消息编码的结果通过sum函数连接起来，形成一个列表，其中包含了整个对话的编码结果。\n",
    "                dialog_tokens: List[int] = sum(\n",
    "                    [\n",
    "                        self.tokenizer.encode(\n",
    "                            f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()}\",\n",
    "                            bos = True,\n",
    "                            eos = True,\n",
    "                        )\n",
    "                        for prompt, answer in zip(dialog[::2], dialog[1::2])\n",
    "                    ],\n",
    "                    []\n",
    "                )\n",
    "            \n",
    "            \n",
    "            # 确保对话的最后一条消息是由用户发送的。如果最后一条消息不是用户的，这将抛出一个错误。\n",
    "            # 这是因为模型是设计用来生成对用户消息的回应的，所以需要用户的消息作为最后一个输入。\n",
    "            assert (dialog[-1][\"role\"] == \"user\"), f\"Last message must be from user, got {dialog[-1]['role']}\"\n",
    "            \n",
    "            # 把这个最后的用户消息添加到dialog_tokens中。在添加时，\n",
    "            # 它使用tokenizer.encode函数把最后一条用户消息转化成一个数字的列表（即tokens）。\n",
    "            dialog_tokens += self.tokenizer.encode(\n",
    "                f\"{B_INST} {(dialog[-1]['content'].strip())} {E_INST}\",\n",
    "                bos = True,\n",
    "                eos = False,\n",
    "            )\n",
    "            \n",
    "            # 将处理好的 dialog_tokens 添加到 prompt_tokens 列表中，这个列表中的每个元素都代表一段对话的tokens。\n",
    "            prompt_tokens.append(dialog_tokens)\n",
    "            \n",
    "        \n",
    "        # 生成一些 tokens\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens = prompt_tokens, # 输入到模型的tokens\n",
    "            max_gen_len = max_gen_len, # 文本的最大长度\n",
    "            temperature = temperature, \n",
    "            top_p = top_p,\n",
    "            logprobs = logprobs,\n",
    "        )\n",
    "        \n",
    "        # 需要返回每个生成token的log概率\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": self.tokenizer.decode(t),\n",
    "                    },\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        \n",
    "        # 如果logprobs为False，则返回一个字典列表。\n",
    "        return [\n",
    "            {\"generation\": {\"role\": \"assistant\",\n",
    "                            \"content\": self.tokenizer.decode(t)}}\n",
    "            for t in generation_tokens\n",
    "        ]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
