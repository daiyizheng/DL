{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlock\n",
    "\n",
    "这个TransformerBlock类的设计允许多个这样的块可以堆叠在一起，形成一个深度的Transformer网络。每个块的输出会被用作下一个块的输入，这样的设计使得Transformer能够处理非常复杂的序列建模任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    标准的Transformer块，其包含一个多头注意力模块 (Attention) 和一个前馈神经网络模块 (FeedForward)\n",
    "    这两个模块之间插入了归一化层 (在这里是RMSNorm)，并使用了残差连接，这两者都有助于改善模型训练的稳定性和性能。\n",
    "    '''\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        \n",
    "        # 一个多头注意力模块，用于对输入执行自注意力操作。\n",
    "        # 这个模块会计算输入的每个元素与其他元素之间的相互关系，并将这些关系用于更新输入。\n",
    "        self.attention = Attention(args)\n",
    "        \n",
    "        # 一个前馈神经网络模块，它由两个线性层和一个SILU激活函数组成。\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim = args.dim,\n",
    "            hidden_dim = 4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        \n",
    "        # 两个都是RMS归一化层，用于对注意力和前馈神经网络的输出进行归一化。\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        # 残差连接\n",
    "        # 将注意力模块的输出与原始输入x相加，形成一个残差连接。这是一种常见的深度学习技术，\n",
    "        # 可以帮助减少训练深层网络时的梯度消失问题。\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), # 对输入x进行归一化，然后将归一化的x传递给注意力模块。\n",
    "            start_pos, # 开始的位置\n",
    "            freqs_cis, # 频率\n",
    "            mask,\n",
    "        )\n",
    "        \n",
    "        # 对结果h进行归一化，然后传递给前馈神经网络模块。\n",
    "        # 前馈神经网络模块对其输入进行进一步的转换，并将输出与h相加，形成另一个残差连接。\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        # 这个out将被用作下一个Transformer块的输入\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        # 基本参数\n",
    "        self.params = params\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = params.vocab_size\n",
    "        # 模型的层数\n",
    "        self.n_layers = params.n_layers\n",
    "        # 这个嵌入层会把每个单词映射到一个高维向量，这个高维向量就是这个单词的嵌入。\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "        # 创建了一个空的模块列表\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        # 添加了n_layers个TransformerBlock到列表中\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "        # 创建了一个RMSNorm层，它用于对输入数据进行归一化处理。\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        \n",
    "        # ColumnParallelLinear层是一个线性层，用于将输入数据的特征从params.dim维映射到params.vocab_size维。\n",
    "        # 这种映射是通过学习一组权重来实现的，权重矩阵的大小为 params.dim x params.vocab_size。\n",
    "        # 简言之，将输入转化为params.vocab_size维的输出，这个输出可以看作是预测每个词汇的概率分布。\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "        # 计算了freqs_cis，这是一个预计算的张量，用于后面的旋转位置嵌入（Rotary Position Embedding）\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_headers, \n",
    "            self.params.max_seq_len * 2,\n",
    "        )\n",
    "        \n",
    "    # 通过torch.inference_mode()装饰器来指示这个方法将用于模型推理，\n",
    "    # 这可以帮助PyTorch优化计算，并在可能的情况下减少内存使用。\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        # 批量大小（_bsz）和序列长度（seqlen）\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        # 词嵌入向量\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        # 根据输入的序列起始位置start_pos和序列长度seqlen，从self.freqs_cis中取出对应的旋转嵌入。\n",
    "        # 这些旋转嵌入将用于后续的Transformer层中，对输入的词嵌入进行旋转操作，以编码位置信息。\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "        \n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            # 模型首先生成了一个掩码（mask），这个掩码被用于transformer层以防止在自注意力机制中考虑到未来的词汇。\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            \n",
    "            # 这是通过填充一个全为负无穷的矩阵，然后使用torch.triu（取上三角）函数，来创建一个遮罩，\n",
    "            # 该遮罩对应的位置上的元素，\n",
    "            # 如果它们代表的词在序列中是在当前词之后的词，则值为负无穷，否则为0。\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "            \n",
    "        # 对每个transformer层，依次将当前的嵌入向量（或者前一层的输出）作为输入，\n",
    "        # 执行该层的前向传播，计算结果将用于下一层的输入。\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "            \n",
    "        # 将最后一层transformer层的输出通过一个规范化（norm）层，然后通过一个全连接层（self.output），\n",
    "        # 转换为最后的模型输出。这个输出的尺寸应该与词汇表的大小相同，因此每个词都有一个对应的分数，\n",
    "        # 这个分数代表模型认为该词是下一个词的可能性。\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
