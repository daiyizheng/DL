{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速开始\n",
    "本指南展示了如何使用 vLLM 来：\n",
    "\n",
    "- 对数据集运行离线批量推理；\n",
    "- 为大型语言模型构建API服务器；\n",
    "- 启动兼容 OpenAI 的 API 服务器。\n",
    "\n",
    "> [默认情况下，vLLM 从HuggingFace](https://huggingface.co/)下载模型。如果您想在以下示例中使用[ModelScope](https://www.modelscope.cn/)中的模型，请设置环境变量：\n",
    "```shell\n",
    "export VLLM_USE_MODELSCOPE=True\n",
    "```\n",
    "\n",
    "## 离线批量推理\n",
    "我们首先展示一个使用 vLLM 对数据集进行离线批量推理的示例。换句话说，我们使用 vLLM 生成输入提示列表的文本。\n",
    "\n",
    "`LLM`从`SamplingParamsvLLM`导入。该类LLM是使用 vLLM 引擎运行离线推理的主类。该类`SamplingParams`指定采样过程的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/slurm/home/yrd/shaolab/daiyizheng/github_project/DL/08-llms/vllm-infer/Untitled-1.ipynb 单元格 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzju_out_debug/slurm/home/yrd/shaolab/daiyizheng/github_project/DL/08-llms/vllm-infer/Untitled-1.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvllm\u001b[39;00m \u001b[39mimport\u001b[39;00m LLM, SamplingParams\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义输入提示列表和生成的采样参数。采样温度设置为0.8，核采样概率设置为0.95。有关采样参数的更多信息，请参阅类定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化 `vLLM` 的引擎，以使用`LLM`类和[OPT-125M](https://arxiv.org/abs/2205.01068) 模型进行离线推理。支持的型号列表可在支持的型号中找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用`llm.generate`以生成输出。它将输入提示添加到 vLLM 引擎的等待队列中，并执行 vLLM 引擎以生成高吞吐量的输出。输出作为对象列表返回`RequestOutput`，其中包括所有输出标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI 兼容服务器\n",
    "vLLM可以部署为实现`OpenAI API`协议的服务器。这使得 vLLM 可以用作使用 OpenAI API 的应用程序的直接替代品。默认情况下，它在 处启动服务器`http://localhost:8000`。您可以使用--host和参数指定地址--port。服务器当前一次托管一个模型（下面命令中的 OPT-125M）并实现列表模型、创建聊天完成和创建完成端点。我们正在积极增加对更多端点的支持。\n",
    "\n",
    "```\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model facebook/opt-125m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，服务器使用存储在标记生成器中的预定义聊天模板。您可以使用`--chat-template`参数覆盖此模板："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model facebook/opt-125m \\\n",
    "    --chat-template ./examples/template_chatml.jinja\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该服务器可以按照与OpenAI API相同的格式进行查询。例如，列出型号："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "您可以传入参数`--api-key`或环境变量`VLLM_API_KEY`，以使服务器能够检查标头中的 API 密钥。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将 OpenAI Completions API 与 vLLM 结合使用\n",
    "通过输入提示查询模型：\n",
    "```shell\n",
    "curl http://localhost:9909/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"/slurm/home/yrd/shaolab/daiyizheng/resources/modelscope/shakechen/Llama-2-7b-chat-hf\",\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"max_tokens\": 7,\n",
    "        \"temperature\": 0\n",
    "    }'\n",
    "```\n",
    "\n",
    "由于该服务器与 `OpenAI API` 兼容，因此您可以将其用作任何使用 `OpenAI API` 的应用程序的直接替代品。例如，查询服务器的另一种方法是通过openaipython 包："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"facebook/opt-125m\",\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "print(\"Completion result:\", completion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Chat API with vLLM\n",
    "vLLM服务器旨在支持OpenAI聊天API，允许您与模型进行动态对话。聊天界面是与模型通信的一种更具交互性的方式，允许存储在聊天历史记录中的来回交换。这对于需要上下文或更详细解释的任务很有用。\n",
    "\n",
    "使用OpenAI Chat API查询模型\n",
    "\n",
    "您可以使用[create chat completion](https://platform.openai.com/docs/api-reference/chat/completions/create)端点在类似聊天的界面中与模型通信\n",
    "\n",
    "```shell\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"facebook/opt-125m\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "        ]\n",
    "    }'\n",
    "```\n",
    "\n",
    "Python客户端示例:使用openai Python包，您还可以以类似聊天的方式与模型通信"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"facebook/opt-125m\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
